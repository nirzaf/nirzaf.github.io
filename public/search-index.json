[{"slug":"advanced-csharp-programming-delegates-events-generics-async-await-and-linq","title":"A Deep Dive into Delegates, Events, Generics, Async/Await, and LINQ","description":"Learn about advanced C# concepts like delegates, events, generics, async/await tasks, and LINQ.","tags":["Csharp","Async","Await","Task"],"pubDate":"2024-10-06","content":" ## Advanced C# Programming: A Deep Dive into Delegates, Events, Generics, Async/Await, and LINQ  This comprehensive guide delves into advanced C# concepts, enhancing your programming proficiency. We'll explore intricate aspects like delegates, events, generics, async/await tasks, and LINQ, providing detailed code examples and best practice guidelines.  We'll also touch upon design patterns, user actions, query operators, and C# attributes.  ### Why Advanced C# Matters: The Abstraction Factor  Advanced C# topics are distinguished by their **abstraction**.  Abstract concepts in programming represent universal patterns that can be implemented to solve various real-world problems. Mastering these concepts leads to:  * Better code reuse * Cleaner, more maintainable code * Enhanced design flexibility and extensibility * Effective unit testing * Improved application performance and efficiency  While these advanced patterns can appear complex initially, the payoff in terms of code quality and efficiency is significant.  ### Example: Leveraging Abstraction with LINQ  A simple example illustrates the power of abstraction. Imagine calculating the total salaries of employees, including bonuses. A traditional approach might involve a `foreach` loop:  ```csharp decimal totalSalary = 0; foreach (IEmployee employee in employees) {     totalSalary += employee.Salary; } Console.WriteLine(totalSalary); ```  Using LINQ, we can achieve the same result with a single line:  ```csharp decimal totalSalary = employees.Sum(e => e.Salary); Console.WriteLine(totalSalary); ```  This conciseness and clarity are hallmarks of advanced C# techniques.  ### Core Advanced C# Concepts  Let's briefly define some key concepts:  * **Delegates:** Type-safe function pointers referencing methods with specific parameters and return types. * **Events:** Special multicast delegates invoked within the declaring class/struct, allowing other classes to subscribe and receive notifications. * **Generics:** Enable designing classes and methods that defer type specification until instantiation, promoting code reuse and type safety. * **Extension Methods:** Add methods to existing types without modification, enhancing code extensibility. * **Lambda Expressions:** Concise representations of anonymous methods, simplifying code. * **LINQ (Language Integrated Query):**  Enables querying strongly typed collections using language keywords and familiar operators. * **Async/Await:** Facilitates asynchronous programming, improving application responsiveness. * **Attributes:** Add metadata to code elements, enabling declarative programming and runtime inspection via reflection. * **Reflection:**  Allows inspecting and interacting with code metadata at runtime.  ### Delegates: Function Pointers for Flexibility  Delegates provide a way to reference and invoke methods dynamically. This is crucial for event handling, asynchronous callbacks, and flexible code design.  A simple example:  ```csharp // Delegate definition public delegate void LogDelegate(string text);  // Method to be referenced by the delegate public static void LogToConsole(string text) {     Console.WriteLine(text); }  // Delegate instantiation and invocation LogDelegate log = new LogDelegate(LogToConsole); log(\"This is a test message.\"); ```  ### Events: The Observer Pattern in Action  Events implement the Observer pattern, enabling loose coupling between components.  An event publisher raises an event, and subscribers react accordingly. Example:  ```csharp // Publisher class public class Counter {     public event EventHandler ThresholdReached;      protected virtual void OnThresholdReached(EventArgs e)     {         ThresholdReached?.Invoke(this, e);     }      public void Count(int threshold)     {         for (int i = 0; i  intList = new List(); intList.Add(10); intList.Add(20); ```   ### Async/Await: Responsive Applications  Async/await simplifies asynchronous programming, making it easier to write responsive applications.  Example:  ```csharp public async Task DownloadStringAsync(string uri) {     using (HttpClient client = new HttpClient())     {         return await client.GetStringAsync(uri);     } } ```  ### LINQ: Querying Data with Ease  LINQ provides a powerful, unified way to query data from various sources.  Example:  ```csharp // Querying a list of employees var highEarners = from e in employees                  where e.AnnualSalary > 50000                  select new { e.FirstName, e.LastName };  foreach (var employee in highEarners) {     Console.WriteLine($\"{employee.FirstName} {employee.LastName}\"); } ```  ### Attributes and Reflection: Metadata Magic  Attributes provide metadata, and reflection allows accessing this metadata at runtime.  Example:  ```csharp // Custom attribute [AttributeUsage(AttributeTargets.Property)] public class RequiredAttribute : Attribute { }  // Using the attribute public class Employee {     [Required]     public string FirstName { get; set; }     [Required]    "},{"slug":"advantages-of-using-mongodb-in-modern-applications","title":"Advantages of Using MongoDB in Modern Applications","description":"Exploring the benefits of MongoDB and how it enhances application development","tags":["Mongodb","Nosql","Database"],"pubDate":"2024-11-19","content":" # Advantages of Using MongoDB in Modern Applications  **MongoDB is a leading NoSQL database that offers flexibility, scalability, and performance for modern applications.**  ## Introduction  In the era of big data and cloud computing, MongoDB has emerged as a popular choice for developers. This article explores the advantages of using MongoDB, its unique features, and how it can enhance application development. As businesses increasingly rely on data-driven decision-making, the need for databases that can handle diverse data types and large volumes of information has grown. MongoDB addresses these needs with its flexible data model and robust performance capabilities.  ## What is MongoDB?  MongoDB is a document-oriented NoSQL database designed for high availability and scalability. It stores data in flexible, JSON-like documents, making it easy to work with complex data structures. Unlike traditional relational databases, MongoDB does not require a predefined schema, allowing developers to modify data structures as needed without downtime. This flexibility is particularly beneficial for applications that need to evolve rapidly in response to changing business requirements.  ## Advantages of MongoDB  ### Flexibility  MongoDB's schema-less design allows for dynamic data models, enabling developers to adapt to changing requirements without downtime. This flexibility is particularly beneficial for startups and agile teams that need to iterate quickly and respond to market changes. For example, an e-commerce platform can easily add new product attributes without altering the existing database schema, allowing for rapid feature deployment.  ### Scalability  With built-in sharding and replication, MongoDB can handle large volumes of data and high traffic loads, making it ideal for growing applications. Sharding allows data to be distributed across multiple servers, ensuring that the database can scale horizontally as demand increases. This capability is crucial for applications that experience variable workloads, such as social media platforms or online gaming services.  ### Performance  MongoDB's indexing and aggregation capabilities provide fast query performance, even with large datasets. The database supports various types of indexes, including compound indexes, geospatial indexes, and text indexes, which optimize query execution and improve application responsiveness. For instance, a location-based service can use geospatial indexing to quickly retrieve nearby points of interest for users.  ### Rich Query Language  MongoDB offers a powerful query language that supports complex queries, aggregations, and data transformations. The aggregation framework allows developers to perform data processing and analysis directly within the database, reducing the need for additional data processing layers. This feature is particularly useful for applications that require real-time analytics, such as financial trading platforms or IoT systems.  ### Community and Ecosystem  With a large community and extensive ecosystem, MongoDB provides a wealth of resources, tools, and integrations for developers. The MongoDB Atlas cloud service offers a fully managed database solution, simplifying deployment and management. Additionally, the MongoDB University provides free online courses to help developers learn and master MongoDB.  ## Use Cases  MongoDB is used across various industries and applications, including:  - **E-commerce**: MongoDB's flexible schema and scalability make it ideal for managing product catalogs, customer data, and order histories. Companies like eBay and Etsy use MongoDB to power their online marketplaces. - **Content Management**: The database's ability to store diverse data types and support full-text search makes it suitable for content management systems. The New York Times uses MongoDB to manage its vast archive of articles and multimedia content. - **IoT**: MongoDB's scalability and real-time data processing capabilities are well-suited for IoT applications that generate large volumes of data. Bosch uses MongoDB to collect and analyze data from connected devices. - **Gaming**: The database's performance and scalability support the high transaction rates and data volumes typical in gaming applications. SEGA uses MongoDB to manage player data and game state information.  ## Comparison with Other Databases  ### Relational Databases  Unlike traditional relational databases, MongoDB does not require a fixed schema, allowing for more flexible data modeling. This flexibility can lead to faster development cycles and easier adaptation to changing requirements. However, it also means that developers need to carefully design their data models to ensure data consistency and integrity.  ### Other NoSQL Databases  Compared to other NoSQL databases, MongoDB offers a rich query language and strong consistency guarantees, making it a versatile choice for a wide range of applications. While some NoSQL databases prioritize "},{"slug":"advantages-of-using-windsurf-ide-for-coding","title":"Advantages of Using Windsurf IDE for Coding","description":"Revolutionizing development workflows with AI-powered assistance","tags":["Development","Tools","Ai"],"pubDate":"2025-04-06","content":" ![Windsurf IDE Interface](https://cdn.thenewstack.io/media/2025/01/5fca8da5-windsurfermain.jpg)  [Windsurf IDE](https://windsurf.com/refer?referral_code=9ba6fb3a9b) is an AI-powered Integrated Development Environment designed to streamline the coding process and enhance developer productivity. By combining state-of-the-art AI features with the familiar VS Code ecosystem, Windsurf offers a suite of tools tailored to modern development needs. Its intuitive interface and intelligent assistance create a seamless coding experience that minimizes errors and facilitates collaboration.  Built by Codeium, Windsurf represents the next generation of development environments where human creativity and machine intelligence blend seamlessly to create a powerful coding experience.  ## Enhanced Productivity and Efficiency  ### Context-Aware AI Flows  Windsurf's AI Flows act as a smart coding companion that understands your project context. This feature anticipates your next moves, provides suggestions tailored to your current task, and automates repetitive coding chores. With real-time code optimization and debugging, developers can maintain their \"flow state\" and focus on solving complex problems rather than being bogged down by routine tasks.  Unlike traditional assistants, Flows allow the developer and AI to operate on the same state at all times, creating a mind-meld experience beyond just an assistant.  ![AI Flows Diagram](https://exafunction.github.io/public/images/flows/flows-diagram.png)  ### Supercomplete – Intelligent Autocomplete  ![Intelligent Autocomplete](https://miro.medium.com/v2/resize:fit:1400/1*ZDaPK2e53FV0kDXAVxAPMg.png)  Unlike traditional autocomplete systems, Windsurf's Supercomplete predicts entire code blocks along with context-aware suggestions. Developers benefit from reduced boilerplate code, fewer errors, and a more fluid coding experience. This intelligent system helps both novice and experienced coders write cleaner and more efficient code.  Supercomplete analyzes what your next action might be, beyond just inserting the next code snippet, creating a truly intuitive coding experience.  ## Simplified Project Management  ### Multi-File Editing and Cascade Management  Managing large codebases can be challenging, but [Windsurf](https://windsurf.com/refer?referral_code=9ba6fb3a9b) simplifies this with its cascade feature. Cascade provides a clear overview of project structures by analyzing file relationships and tracking real-time changes. In turn, multi-file editing ensures consistent modifications across the project, reducing cognitive load and streamlining debugging processes.  ![Cascade Management Feature](https://cdn.thenewstack.io/media/2025/01/5fca8da5-windsurfermain.jpg)  > **Cascade's Key Features:** > - Full contextual awareness of your codebase > - Suggestion and execution of terminal commands > - Picks up where you left off with automatic reasoning > - Coherent multi-file edits with deep context awareness  ### Seamless Integration with VS Code  ![VS Code Integration](https://miro.medium.com/v2/resize:fit:1200/1*c8ivcNbXdmPh7md3y0_SQg.gif)  Built on the familiar Visual Studio Code platform, Windsurf allows developers to import existing settings, plugins, and extensions effortlessly. This seamless integration ensures that transitioning to Windsurf is smooth, retaining the flexibility and extensive plugin support that developers already love.  During your initial installation, you have the option to import all your extensions and settings from VS Code or Cursor, or you can start fresh with a clean setup tailored to your preferences.  ## Intelligent Assistance for Smarter Coding  ### Real-Time Code Optimization  ![Real-Time Code Optimization](https://www.geeky-gadgets.com/wp-content/uploads/2024/11/windsurf-ai-ide-coding-assistant-overview.webp)  Windsurf IDE optimizes code on the fly by employing multiple AI models. It can refactor functions, improve performance, and generate documentation as you code, minimizing manual intervention and enhancing overall code quality.  ### Inline AI and Terminal Chat  ![Inline AI Edits](https://codeium.com/static/images/windsurf/feature-command.png)  Press Cmd + I in your editor to request changes, generate documentation, or modify specific code segments without affecting the entire file. Terminal chat using inline commands enhances error resolution and command execution.  ### Intelligent Code Lenses  ![Code Lenses](https://codeium.com/static/images/windsurf/feature-codelenses.png)  Available next to breadcrumbs, code lenses let you understand or refactor code with one click. This intelligent feature provides context-aware actions that enhance code comprehension and maintenance.  ### Model Context Protocol (MCP)  ![Model Context Protocol](https://codeium.com/static/images/windsurf/feature-mcp.png)  Windsurf's Model Context Protocol allows you to enhance your AI workflows by connecting to custom tools and services. This powerful feature extends the IDE's "},{"slug":"angular-with-dotnet-core-api","title":"Angular 18 and .NET Core API Authorization","description":"Learn how to implement secure authorization between an Angular 18 frontend and a .NET Core API backend using JWT tokens and role-based access control.","tags":["Angular","Dotnet","Api","Authorization","Security"],"pubDate":"","content":" # Angular 18 and .NET Core API Authorization  Building secure web applications requires proper authorization between frontend and backend systems. In this post, we'll explore how to implement robust authorization between an Angular 18 frontend and a .NET Core API backend.  ## Setting Up the .NET Core API  First, let's set up our .NET Core API with JWT authentication:  ```csharp // Startup.cs public void ConfigureServices(IServiceCollection services) {     // Add JWT Authentication     services.AddAuthentication(JwtBearerDefaults.AuthenticationScheme)         .AddJwtBearer(options =>         {             options.TokenValidationParameters = new TokenValidationParameters             {                 ValidateIssuer = true,                 ValidateAudience = true,                 ValidateLifetime = true,                 ValidateIssuerSigningKey = true,                 ValidIssuer = Configuration[\"Jwt:Issuer\"],                 ValidAudience = Configuration[\"Jwt:Audience\"],                 IssuerSigningKey = new SymmetricSecurityKey(                     Encoding.UTF8.GetBytes(Configuration[\"Jwt:Key\"]))             };         });      // Add Authorization     services.AddAuthorization(options =>     {         options.AddPolicy(\"AdminOnly\", policy => policy.RequireRole(\"Admin\"));         options.AddPolicy(\"UserAccess\", policy => policy.RequireRole(\"User\", \"Admin\"));     });      services.AddControllers(); }  public void Configure(IApplicationBuilder app, IWebHostEnvironment env) {     // Other middleware...          app.UseAuthentication();     app.UseAuthorization();          // Other middleware... } ```  ## Creating a JWT Token Service  Next, let's create a service to generate JWT tokens:  ```csharp public class TokenService : ITokenService {     private readonly IConfiguration _configuration;          public TokenService(IConfiguration configuration)     {         _configuration = configuration;     }          public string GenerateJwtToken(User user)     {         var claims = new List         {             new Claim(ClaimTypes.NameIdentifier, user.Id.ToString()),             new Claim(ClaimTypes.Name, user.Username),             new Claim(ClaimTypes.Email, user.Email)         };                  // Add roles as claims         foreach (var role in user.Roles)         {             claims.Add(new Claim(ClaimTypes.Role, role));         }                  var key = new SymmetricSecurityKey(Encoding.UTF8.GetBytes(_configuration[\"Jwt:Key\"]));         var creds = new SigningCredentials(key, SecurityAlgorithms.HmacSha256);                  var token = new JwtSecurityToken(             issuer: _configuration[\"Jwt:Issuer\"],             audience: _configuration[\"Jwt:Audience\"],             claims: claims,             expires: DateTime.Now.AddDays(1),             signingCredentials: creds         );                  return new JwtSecurityTokenHandler().WriteToken(token);     } } ```  ## Implementing Protected API Endpoints  Now, let's create some protected API endpoints:  ```csharp [ApiController] [Route(\"api/[controller]\")] public class ResourcesController : ControllerBase {     [HttpGet(\"public\")]     public IActionResult GetPublicData()     {         return Ok(new { message = \"This is public data\" });     }          [HttpGet(\"user\")]     [Authorize(Policy = \"UserAccess\")]     public IActionResult GetUserData()     {         return Ok(new { message = \"This is protected user data\" });     }          [HttpGet(\"admin\")]     [Authorize(Policy = \"AdminOnly\")]     public IActionResult GetAdminData()     {         return Ok(new { message = \"This is protected admin data\" });     } } ```  ## Setting Up Angular 18 Authentication  On the Angular side, we need to set up authentication and handle JWT tokens:  ```typescript // auth.service.ts @Injectable({   providedIn: 'root' }) export class AuthService {   private currentUserSubject = new BehaviorSubject(null);   public currentUser$ = this.currentUserSubject.asObservable();      constructor(private http: HttpClient) {     // Check if we have a token in local storage     const token = localStorage.getItem('token');     if (token) {       const user = this.getUserFromToken(token);       this.currentUserSubject.next(user);     }   }      login(username: string, password: string): Observable {     return this.http.post(`${environment.apiUrl}/auth/login`, { username, password })       .pipe(         tap(response => {           // Save token to local storage           localStorage.setItem('token', response.token);                      // Update current user           const user = this.getUserFromToken(response.token);           this.currentUserSubject.next(user);         })       );   }      logout(): void {     localStorage.removeItem('token');     this.currentUserSubject.next(null);   }      getToken(): string | null {     return localStorage.getItem('token');   }      isLoggedIn(): boolean {     return !!this.getToken();   }      hasRole(role: string): boolean {     const user = thi"},{"slug":"asynchronous-programming-with-async-await-task-in-csharp","title":"Asynchronous Programming With Async Await Task in C#","description":"Learn about asynchronous programming in C# using async, await, and Task.","tags":["Csharp","Async","Await","Task"],"pubDate":"2023-05-03","content":" # Asynchronous Programming With Async Await Task in C#  **C# and .NET Framework (4.5 & Core) supports asynchronous programming using some native functions, classes, and reserved keywords.**  Before we see what is asynchronous programming, let's understand what is synchronous programming using the following console example.  ## Synchronous Programming Example  ```csharp static void Main(string[] args) {     LongProcess();      ShortProcess(); }  static void LongProcess() {     Console.WriteLine(\"LongProcess Started\");      //some code that takes long execution time     System.Threading.Thread.Sleep(4000); // hold execution for 4 seconds      Console.WriteLine(\"LongProcess Completed\"); }  static void ShortProcess() {     Console.WriteLine(\"ShortProcess Started\");      //do something here      Console.WriteLine(\"ShortProcess Completed\"); } ```  Output:  ```csharp LongProcess Started LongProcess Completed ShortProcess Started ShortProcess Completed ```  In the above example, the `LongProcess()` method takes 4 seconds to complete. The `ShortProcess()` method takes a very short time to complete. But the `ShortProcess()` method has to wait for the `LongProcess()` method to complete. This is called synchronous programming.  In synchronous programming, the program execution waits for the current task to complete before moving on to another task. This is the default behavior of the program.  The above program executes synchronously. It means execution starts from the Main() method wherein it first executes the LongProcess() method and then ShortProcess() method. During the execution, an application gets blocked and becomes unresponsive (You can see this in Windows-based applications mainly). This is called synchronous programming where execution does not go to next line until the current line executed completely.  ## What is Asynchronous Programming?  In asynchronous programming, the code gets executed in a thread without having to wait for an I/O-bound or long-running task to finish. For example, in the asynchronous programming model, the LongProcess() method will be executed in a separate thread from the thread pool, and the main application thread will continue to execute the next statement.  Microsoft recommends [Task-based Asynchronous Pattern](https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/concepts/async/task-asynchronous-programming-model) to implement asynchronous programming in the [.NET Framework](https://dotnet.microsoft.com/en-us/download/dotnet-framework) or [.NET Core](https://dotnet.microsoft.com/en-us/download/dotnet-framework) applications using ```async``` , await keywords and ```Task``` or ```Task``` class.  Now let's rewrite the above example in asynchronous pattern using ```async``` keyword.  ## Asynchronous Programming Example  ```csharp static async Task Main(string[] args) {     LongProcess();      ShortProcess(); }  static async void LongProcess() {     Console.WriteLine(\"LongProcess Started\");      await Task.Delay(4000); // hold execution for 4 seconds      Console.WriteLine(\"LongProcess Completed\");  }  static void ShortProcess() {     Console.WriteLine(\"ShortProcess Started\");      //do something here      Console.WriteLine(\"ShortProcess Completed\"); } ```  Output:  ```csharp LongProcess Started ShortProcess Started ShortProcess Completed LongProcess Completed ```  In the above example, the Main() method is marked by the async keyword, and the return type is Task. The async keyword marks the method as asynchronous. Note that all the methods in the method chain must be async in order to implement asynchronous programming. So, the Main() method must be async to make child methods asynchronous.  The LongProcess() method is also marked with the async keyword which makes it asynchronous. The await Task.Delay(4000); holds the thread execute for 4 seconds.  Now, the program starts executing from the async Main() method in the main application thread. The async LongProcess() method gets executed in a separate thread and the main application thread continues execution of the next statement which calls ShortProcess() method and does not wait for the LongProcess() to complete.  ## async, await, and Task  Use async along with await and Task if the async method returns a value back to the calling code. We used only the async keyword in the above program to demonstrate the simple asynchronous void method.  The await keyword waits for the async method until it returns a value. So the main application thread stops there until it receives a return value.  The Task class represents an asynchronous operation and ```Task``` generic class represents an operation that can return a value. In the above example, we used await Task.Delay(4000) that started async operation that sleeps for 4 seconds and await holds a thread until 4 seconds.  The following demonstrates the async method that returns a value.  ```csharp static async Task Main(string[] args) {     Task result = LongProcess();      Sho"},{"slug":"beyond-the-basics-mastering-advanced-features-in-dotnet","title":"Beyond the Basics: Mastering Advanced Features in .NET","description":"Explore advanced .NET features such as reflection, dependency injection, asynchronous programming, and performance optimization to elevate your development skills.","tags":["Dotnet","Advanced","Reflection","Dependency Injection","Async","Performance"],"pubDate":"2025-01-11","content":" The .NET ecosystem is renowned for its versatility and power, enabling developers to build robust, scalable, and high-performance applications. While foundational knowledge is essential, true mastery comes from understanding and leveraging advanced features. This article delves into some of the most impactful advanced capabilities in .NET, including reflection, dependency injection, asynchronous programming, and performance optimization.  ## Reflection: Inspecting and Modifying Code at Runtime  Reflection is a powerful feature in .NET that allows you to inspect assemblies, types, and members at runtime. This capability is invaluable for scenarios such as plugin architectures, serialization frameworks, and dynamic type discovery.  ### Practical Example: Dynamic Type Instantiation  Suppose you need to instantiate a class based on its name at runtime:  ```csharp using System; using System.Reflection;  public class Example {     public void Run()     {         string typeName = \"MyNamespace.MyClass\";         Type type = Type.GetType(typeName);         if (type != null)         {             object instance = Activator.CreateInstance(type);             Console.WriteLine($\"Created instance of {type.FullName}\");         }     } } ```  Reflection should be used judiciously, as it can introduce performance overhead and reduce compile-time safety. However, when applied appropriately, it unlocks dynamic behaviors that are otherwise impossible.  ## Dependency Injection: Decoupling for Testability and Flexibility  Dependency Injection (DI) is a core principle in modern .NET development, promoting loose coupling and enhancing testability. The built-in DI container in ASP.NET Core makes it straightforward to register and resolve dependencies.  ### Registering and Resolving Services  ```csharp public interface IMessageService {     void Send(string message); }  public class EmailService : IMessageService {     public void Send(string message)     {         // Send email logic     } }  // In Startup.cs or Program.cs services.AddScoped(); ```  By depending on abstractions rather than concrete implementations, your code becomes more modular and easier to maintain. Advanced scenarios include using factories, decorators, and custom scopes to tailor DI to complex requirements.  ## Asynchronous Programming: Maximizing Responsiveness and Throughput  Asynchronous programming with `async` and `await` is essential for building scalable .NET applications, especially web APIs and UI applications. It enables non-blocking operations, improving responsiveness and resource utilization.  ### Example: Asynchronous Data Access  ```csharp public async Task> GetUsersAsync() {     using var context = new AppDbContext();     return await context.Users.ToListAsync(); } ```  Advanced usage involves understanding synchronization contexts, avoiding deadlocks, and leveraging parallelism with `Task.WhenAll` or `Parallel.ForEachAsync` for high-throughput scenarios.  ## Performance Optimization: Profiling and Tuning  Performance is a critical aspect of advanced .NET development. Profiling tools such as Visual Studio Profiler, dotTrace, and BenchmarkDotNet help identify bottlenecks. Key techniques include:  - **Minimizing allocations**: Use value types and pooling where appropriate. - **Efficient collections**: Choose the right data structures (e.g., `Span`, `ArrayPool`, `Dictionary`). - **Compiled queries**: In Entity Framework, use compiled queries to reduce query translation overhead.  ### Example: Using BenchmarkDotNet  ```csharp [MemoryDiagnoser] public class StringConcatBenchmarks {     [Benchmark]     public string UsingPlusOperator() => \"Hello\" + \" \" + \"World\";      [Benchmark]     public string UsingStringBuilder()     {         var sb = new StringBuilder();         sb.Append(\"Hello\");         sb.Append(\" \");         sb.Append(\"World\");         return sb.ToString();     } } ```  BenchmarkDotNet provides detailed insights into execution time and memory usage, guiding targeted optimizations.  ## Advanced Language Features: Pattern Matching, Records, and More  C# continues to evolve, introducing features that enhance expressiveness and safety. Pattern matching, records, and nullable reference types are just a few examples.  ### Pattern Matching Example  ```csharp object shape = new Circle(5); if (shape is Circle { Radius: > 0 } c) {     Console.WriteLine($\"Circle with radius {c.Radius}\"); } ```  These features enable more concise, readable, and robust code, especially when dealing with complex data models and business logic.  Mastering advanced features in .NET empowers you to build applications that are not only functional but also maintainable, performant, and scalable. By embracing reflection, dependency injection, asynchronous programming, and performance optimization, you elevate your development practice and unlock the full potential of the .NET platform. Stay curious, experiment with new features, and continuously refine your skills to stay "},{"slug":"blazor-web-api-with-jwt-auth","title":"Microsoft Blazor WebAssembly with JWT Authentication in .NET 8","description":"A guide on how to implement a JWT Authentication system into a .NET 8 Web API project that uses Microsoft's Blazor WebAssembly.","tags":["Blazor","Jwt","Dotnet8"],"pubDate":"2024-10-01","content":" # Microsoft Blazor WebAssembly with JWT Authentication in .NET 8  **I would like to share a guide on how to implement a JWT Authentication system into a .NET 8 Web API project that uses Microsoft's Blazor WebAssembly, but this same guide can be used for regular ASP.NET Core Web API's.**  If you have not heard of Blazor, I encourage you to take a look at it. In a nutshell, it allows you to write client-side and server-side code using just C#, take a minute to let that sink in… This means no JavaScript needed to write UI, well… there are ways to still use JavaScript using the JavaScript interop if there are no other libraries available in C#.  I hope you find this guide useful and I will post the source code onto [GitHub](https://github.com/nirzaf/DotnetCoreJwtAuthentication.git).  ## Assumptions  - You have Visual Studio 2022 (any edition) v17.8 or later. If you are using anything else then at least have knowledge of the dotnet command line. - You know how to use the NuGet package manager - You know C# and how to build a basic web project. - You know what JWT tokens are and why you have chosen to use them  ## Let's begin  First off, ensure you have the latest .NET 8 SDK installed. You can download it from the [official .NET website](https://dotnet.microsoft.com/download/dotnet/8.0).  Create a new Blazor WebAssembly project with ASP.NET Core hosted option. At the time of writing this guide, you can choose \"Individual Accounts\" for authentication when creating the project.  Next step is to install a few NuGet packages into our Server project:  - Microsoft.AspNetCore.Authentication.JwtBearer  Your Server.csproj file should look similar to this:  ```xml          net8.0     enable     enable                                        ```  Next, we need to setup our Program.cs file in the Server project. In .NET 8, we use the new minimal hosting model, so our setup will look a bit different:  ```csharp using Microsoft.AspNetCore.Authentication.JwtBearer; using Microsoft.IdentityModel.Tokens; using System.Text;  var builder = WebApplication.CreateBuilder(args);  builder.Services.AddAuthentication(JwtBearerDefaults.AuthenticationScheme)     .AddJwtBearer(options =>     {         options.TokenValidationParameters = new TokenValidationParameters         {             ValidateIssuer = true,             ValidateAudience = true,             ValidateLifetime = true,             ValidateIssuerSigningKey = true,             ValidIssuer = builder.Configuration[\"Jwt:Issuer\"],             ValidAudience = builder.Configuration[\"Jwt:Audience\"],             IssuerSigningKey = new SymmetricSecurityKey(Encoding.UTF8.GetBytes(builder.Configuration[\"Jwt:Key\"]))         };     });  builder.Services.AddControllersWithViews(); builder.Services.AddRazorPages();  var app = builder.Build();  // Configure the HTTP request pipeline. if (app.Environment.IsDevelopment()) {     app.UseWebAssemblyDebugging(); } else {     app.UseExceptionHandler(\"/Error\");     app.UseHsts(); }  app.UseHttpsRedirection(); app.UseBlazorFrameworkFiles(); app.UseStaticFiles();  app.UseRouting();  app.UseAuthentication(); app.UseAuthorization();  app.MapRazorPages(); app.MapControllers(); app.MapFallbackToFile(\"index.html\");  app.Run(); ```  Now create an \"appsettings.json\" file in the root of your Server project and open it. Add in the \"Jwt\" json to setup the token:  ```json {   \"Jwt\": {     \"Key\": \"ThisismySecretKey\",     \"Issuer\": \"Test.com\",     \"Audience\": \"Test.com\"   },   \"ConnectionStrings\": {     \"DefaultConnection\": \"Server=(localdb)\\\\mssqllocaldb;Database=aspnet-BlazorApp-53D9B3A5-4C2A-4A5A-8A5A-4C2A4A5A8A5A;Trusted_Connection=True;MultipleActiveResultSets=true\"   } } ```  Next, we'll create a JwtTokenService. First, let's create an interface:  ```csharp public interface IJwtTokenService {     string BuildToken(string email); } ```  Then create the implementation:  ```csharp using Microsoft.IdentityModel.Tokens; using System.IdentityModel.Tokens.Jwt; using System.Security.Claims; using System.Text;  public class JwtTokenService : IJwtTokenService {     private readonly IConfiguration _configuration;      public JwtTokenService(IConfiguration configuration)     {         _configuration = configuration;     }      public string BuildToken(string email)     {         var claims = new[]         {             new Claim(JwtRegisteredClaimNames.Sub, email),             new Claim(JwtRegisteredClaimNames.Jti, Guid.NewGuid().ToString())         };          var key = new SymmetricSecurityKey(Encoding.UTF8.GetBytes(_configuration[\"Jwt:Key\"]));         var creds = new SigningCredentials(key, SecurityAlgorithms.HmacSha256);          var token = new JwtSecurityToken(             issuer: _configuration[\"Jwt:Issuer\"],             audience: _configuration[\"Jwt:Audience\"],             claims: claims,             expires: DateTime.Now.AddMinutes(30),             signingCredentials: creds);          return new JwtSecurityTokenHandler().WriteToken(token);     } } ```  Don't forget"},{"slug":"building-an-angular-project-with-bootstrap-4-and-firebase","title":"Building an Angular Project With Bootstrap 5 and Firebase","description":"Learn how to start your Angular 17 Project from scratch and add Bootstrap 5 and the Firebase library to your application.","tags":["Angular","Bootstrap","Firebase"],"pubDate":"2024-10-01","content":" # Building an Angular Project With Bootstrap 5 and Firebase  **In this tutorial, I'll show you how to start your Angular 17 Project from scratch and add Bootstrap 5 and the Firebase library to your application. This is the perfect application skeleton for your web application project.**  ## Part 1: Setting Up The Project  ### Setting Up The Angular Project  First we need to setup an Angular project. This is done by using Angular CLI (https://cli.angular.io/). If you have not installed Angular CLI on your system first make sure that the command line interface is installed by executing the following command:  ```bash $ npm install -g @angular/cli@latest ```  Having installed Angular CLI you can now make use of the ng command. By using this command we're able to initiate a new Angular project:  ```bash $ ng new ng-fb-bootstrap ```  To initiate a new project we need to use the command line parameter new and specify the name of the new project.  Change into the newly created project folder:  ```bash $ cd ng-fb-bootstrap ```  In the project folder you can execute the following command to launch the development web server:  ```bash $ ng serve ```  The server is started and the application can be accessed on http://localhost:4200 as you can see in the following screenshot:    ### Setting up Firebase  Next step is to include Firebase in our project. Two steps are needed here: 1. First, a new Firebase project has to be created in the Firebase back-end and the corresponding project settings have to be made available within the Angular application 2. Second, the Firebase and AngularFire libraries have to be added to the project  ### Creating A Firebase Project  To create a new Firebase project go to the Firebase website https://firebase.google.com and create a new account / sign in with your Google credentials. After you're being logged in successfully you can click on the link Go To Console to access the Firebase back-end:  Here you can click on Add project to add a new Firebase project or select one of the existing projects. You're taken to the project console next:    Click on the link Add Firebase to your web app to access the following dialog:    Here you can find the JavaScript code which is needed to initialize the Firebase project for your website. However, to include this Firebase configuration in the Angular project we do only need a part of that code snippet. Copy the key-value pairs inside the config object and insert those pairs inside the file `environments/environment.ts` in the following way:  ```typescript export const environment = {   production: false,   firebase: {     apiKey: \"[...]\",     authDomain: \"[...]\",     projectId: \"[...]\",     storageBucket: \"[...]\",     messagingSenderId: \"[...]\",     appId: \"[...]\"   } }; ```  The key-value pairs are inserted into a new property named firebase. The same needs to be inserted into `environments/environment.prod.ts`:  ```typescript export const environment = {   production: true,   firebase: {     apiKey: \"[...]\",     authDomain: \"[...]\",     projectId: \"[...]\",     storageBucket: \"[...]\",     messagingSenderId: \"[...]\",     appId: \"[...]\"   } }; ```  That's needed to make the Firebase settings available whether we're building during development or for production.  ### Adding Libraries  Next, let's add the libraries to our project by executing the following command:  ```bash $ ng add @angular/fire ```  That command is downloading and installing the @angular/fire package, which includes both the Firebase SDK and AngularFire library.  Insert the following import statements into `app.module.ts`:  ```typescript    ```  Also add the Firebase modules to the imports array of the @NgModule decorator in the following way:  ```typescript @NgModule({   declarations: [     AppComponent,     AppNavbarComponent   ],   imports: [     BrowserModule,     provideFirebaseApp(() => initializeApp(environment.firebase)),     provideFirestore(() => getFirestore()),     provideAuth(() => getAuth())   ],   providers: [],   bootstrap: [AppComponent] }) export class AppModule { } ```  Please note that we're using the new modular SDK approach for Firebase. To gain access to the environment object you need to add the following import statement as well:  ```typescript  ```  ### Adding Bootstrap 5  Let's add the Bootstrap 5 library to our project by using NPM:  ```bash $ npm install bootstrap@5.3.2 ```  To make the Bootstrap CSS classes available for the components in our project we need to include the Bootstrap CSS file from `node_modules/bootstrap/dist/css/bootstrap.css` in our project. To do so add the following line in file `angular.json` under the `styles` array:  ```json \"styles\": [   \"node_modules/bootstrap/dist/css/bootstrap.min.css\",   \"src/styles.css\" ], ```  ### Bootstrap Starter Template  To add a Bootstrap user interface to our sample application we'll make use of the Bootstrap Starter Template which is available at https://getbootstrap.com/docs/5.3/examples/:  To setup a"},{"slug":"building-modern-web-ui-with-blazor-in-2025","title":"Modern Web UIs with Blazor in 2025: State Management & Component Libraries","description":"Explore the latest advancements in Blazor development for 2025, focusing on state management, popular component libraries like MudBlazor and Radzen, and key performance optimization techniques.","tags":["Blazor",".Net","Web Development","Ui","State Management","Mudblazor","Radzen","Performance","Asp.net Core"],"pubDate":"2025-04-06","content":" # Building Modern Web UIs with Blazor in 2025: State Management, Component Libraries, and Performance Optimization  As we move through 2025, Microsoft's Blazor framework has evolved into a mature and powerful platform for building modern web applications. This article explores the latest advancements in Blazor development, focusing on three critical aspects: state management strategies, component libraries (with special attention to MudBlazor and Radzen), and performance optimization techniques that help Blazor applications run smoothly in production environments.  ## The State of Blazor in 2025  Blazor has come a long way since its introduction, and in 2025, it stands as a compelling alternative to JavaScript frameworks for building web applications. With the release of .NET 10, Blazor has gained several key enhancements:  *   **Improved Rendering Engine:** The rendering engine has been optimized to handle more complex UI updates with less overhead. *   **Enhanced Reconnection UI:** The Blazor Web App template now includes a built-in `ReconnectModal` component for improved handling of dropped connections. *   **Better Navigation:** Smoother navigation that avoids full-page flickers when moving between pages. *   **Reduced Memory Footprint:** Significant improvements in memory management for better performance in resource-constrained environments. *   **Expanded Component Ecosystem:** A rich ecosystem of both first-party and third-party component libraries.  These improvements have positioned Blazor as a robust framework for developing modern web applications using C# and .NET instead of JavaScript.  ## Blazor Rendering Models  Before diving into state management and component libraries, it's essential to understand the different rendering models available in Blazor as of 2025, as they significantly impact how you approach state management:  ### Server Rendering  Blazor Server executes your components on the server and maintains a real-time connection with the client using SignalR. Updates are sent to the browser over this connection.  ```csharp // In Program.cs var builder = WebApplication.CreateBuilder(args); builder.Services.AddRazorComponents()     .AddInteractiveServerComponents(); ```  ### WebAssembly Rendering  Blazor WebAssembly runs your components directly in the browser using WebAssembly, offering a true client-side single-page application (SPA) experience.  ```csharp // In Program.cs var builder = WebApplication.CreateBuilder(args); builder.Services.AddRazorComponents()     .AddInteractiveWebAssemblyComponents(); ```  ### Auto Render Mode  Introduced in .NET 8 and enhanced in .NET 10, the Auto render mode combines both approaches, initially using server-side rendering for fast startup and then transitioning to WebAssembly once the WASM runtime is downloaded.  ```razor // In App.razor                                // In MainLayout.razor @inherits LayoutComponentBase @rendermode InteractiveAuto ```  This hybrid model provides the best of both worlds: fast initial rendering with server-side execution, followed by client-side execution after the WebAssembly runtime is loaded.  ## State Management Strategies  State management is a crucial aspect of building complex web applications. In 2025, Blazor offers several approaches to state management, from simple to sophisticated:  ### 1. Component State  The simplest form of state management in Blazor is component-local state. Each component maintains its own state, which is lost when the component is unmounted.  ```razor @page \"/counter\"  Counter  Current count: @currentCount  Click me  @code {     private int currentCount = 0;      private void IncrementCount()     {         currentCount++;     } } ```  ### 2. Cascading Parameters  For parent-child component communication, Blazor provides cascading parameters, which allow parent components to pass data down to all nested components.  ```razor // In parent component         @code {     private ThemeState themeState = new ThemeState { IsDarkMode = true }; }  // In child component @code {     [CascadingParameter]     private ThemeState ThemeState { get; set; } }  public class ThemeState {     public bool IsDarkMode { get; set; } } ```  ### 3. Service-Based State Management  For more complex applications, a dependency injection-based approach using services is a popular choice:  ```csharp // Create a state service public class CounterState {     private int _count = 0;     public int Count => _count;      public event Action? OnStateChanged;      public void IncrementCount()     {         _count++;         NotifyStateChanged();     }      private void NotifyStateChanged() => OnStateChanged?.Invoke(); } ```  ```csharp // Register it as a singleton in Program.cs builder.Services.AddSingleton(); ```  ```razor // Use it in a component @page \"/counter\" @inject CounterState State @implements IDisposable  Counter  Current count: @State.Count  Click me  @code {     protected override void OnInitialized()     {    "},{"slug":"connecting-python-with-supabase","title":"Connecting Python Apps with Supabase - Complete Guide","description":"Learn how to integrate Supabase with Python applications for authentication, database operations, storage, and real-time features.","tags":["Python","Supabase","Database","Backend","Api"],"pubDate":"2023-09-12","content":" Supabase is an open-source Firebase alternative that provides a suite of tools for building modern applications. It offers authentication, database, storage, and real-time capabilities, all accessible through a simple API. In this guide, we'll explore how to integrate Supabase with Python applications.  ## Getting Started with Supabase and Python  ### Installation  First, install the Supabase Python client:  ```bash pip install supabase ```  ### Initializing the Client  ```python from supabase import create_client  # Initialize the Supabase client url = \"https://your-project-id.supabase.co\" key = \"your-supabase-api-key\" supabase = create_client(url, key) ```  ## Authentication  Supabase provides multiple authentication methods. Here's how to implement them in Python:  ### Email and Password Authentication  ```python # Sign up a new user def sign_up(email, password):     response = supabase.auth.sign_up({         \"email\": email,         \"password\": password     })     return response  # Sign in an existing user def sign_in(email, password):     response = supabase.auth.sign_in_with_password({         \"email\": email,         \"password\": password     })     return response  # Sign out def sign_out(jwt):     supabase.auth.sign_out(jwt) ```  ### Social Authentication  For social authentication (like Google, GitHub, etc.), you'll need to handle the OAuth flow:  ```python # Generate the OAuth URL def get_oauth_url(provider):     response = supabase.auth.sign_in_with_oauth({         \"provider\": provider  # e.g., \"google\", \"github\"     })     return response[\"url\"]  # Handle the OAuth callback def handle_oauth_callback(code):     response = supabase.auth.exchange_code_for_session({         \"code\": code     })     return response ```  ## Database Operations  Supabase is built on PostgreSQL, and the Python client provides a simple interface for database operations:  ### Fetching Data  ```python # Get all rows from a table def get_all_items(table_name):     response = supabase.table(table_name).select(\"*\").execute()     return response.data  # Get a specific row by ID def get_item_by_id(table_name, item_id):     response = supabase.table(table_name).select(\"*\").eq(\"id\", item_id).execute()     return response.data[0] if response.data else None  # Query with filters def get_filtered_items(table_name, column, value):     response = supabase.table(table_name).select(\"*\").eq(column, value).execute()     return response.data ```  ### Inserting Data  ```python # Insert a new row def insert_item(table_name, data):     response = supabase.table(table_name).insert(data).execute()     return response.data ```  ### Updating Data  ```python # Update a row def update_item(table_name, item_id, data):     response = supabase.table(table_name).update(data).eq(\"id\", item_id).execute()     return response.data ```  ### Deleting Data  ```python # Delete a row def delete_item(table_name, item_id):     response = supabase.table(table_name).delete().eq(\"id\", item_id).execute()     return response.data ```  ### Advanced Queries  ```python # Join tables def get_items_with_related_data(table_name, related_table):     response = supabase.table(table_name).select(f\"*, {related_table}(*)\").execute()     return response.data  # Pagination def get_paginated_items(table_name, page, page_size):     start = (page - 1) * page_size     end = page * page_size - 1     response = supabase.table(table_name).select(\"*\").range(start, end).execute()     return response.data  # Full-text search def search_items(table_name, column, query):     response = supabase.table(table_name).select(\"*\").textSearch(column, query).execute()     return response.data ```  ## Storage  Supabase provides a storage solution for files. Here's how to use it:  ```python # Upload a file def upload_file(bucket, file_path, file_name):     with open(file_path, \"rb\") as f:         response = supabase.storage.from_(bucket).upload(file_name, f)     return response  # Download a file def download_file(bucket, file_name, save_path):     response = supabase.storage.from_(bucket).download(file_name)     with open(save_path, \"wb\") as f:         f.write(response)     return save_path  # Get a public URL for a file def get_public_url(bucket, file_name):     return supabase.storage.from_(bucket).get_public_url(file_name)  # List files in a bucket def list_files(bucket, path=\"\"):     response = supabase.storage.from_(bucket).list(path)     return response  # Delete a file def delete_file(bucket, file_name):     response = supabase.storage.from_(bucket).remove([file_name])     return response ```  ## Real-time Subscriptions  Supabase allows you to subscribe to changes in your database:  ```python import asyncio from supabase import create_client  async def subscribe_to_changes(table_name):     # Initialize the Supabase client     url = \"https://your-project-id.supabase.co\"     key = \"your-supabase-api-key\"     supabase = create_client(url, key)          # Create a channel to listen for changes     ch"},{"slug":"crafting-cross-platform-dotnet-maui-experiences-with-blazor-hybrid-and-ai","title":"Crafting Cross-Platform .NET MAUI Experiences with Blazor Hybrid & AI","description":"Learn how to build sophisticated cross-platform UIs with .NET MAUI and Blazor Hybrid, then integrate AI capabilities to create intelligent user experiences using a shared C# codebase.","tags":["Maui","Blazor Hybrid","Ai","Cross-platform","C#",".Net","Ml.net","Mobile Development"],"pubDate":"2025-05-10","content":" In the rapidly evolving landscape of cross-platform development, .NET MAUI (Multi-platform App UI) has emerged as Microsoft's unified framework for building native mobile and desktop apps with C# and XAML. When combined with Blazor Hybrid and AI capabilities, it creates a powerful ecosystem for developing sophisticated, intelligent applications that run seamlessly across iOS, Android, macOS, and Windows.  This article explores how to leverage Blazor Hybrid within .NET MAUI applications and integrate AI features to create truly next-generation cross-platform experiences.  ## Understanding .NET MAUI and Blazor Hybrid  ### What is .NET MAUI?  .NET MAUI is Microsoft's evolution of Xamarin.Forms, providing a unified framework for building native mobile and desktop apps with a single codebase. MAUI allows developers to use C# and XAML to create applications that run natively on iOS, Android, macOS, and Windows. It offers native UI controls for each platform, ensuring high performance and a consistent user experience across devices. MAUI is now considered the standard for .NET cross-platform development, with significant improvements in .NET 8 and .NET 9 [[1]](https://www.avidclan.com/blog/blazor-hybrid-with-dot-net-maui-build-cross-platform-web-and-native-apps-in-2025/).  ### What is Blazor Hybrid?  Blazor Hybrid combines the best of both worlds:  - **Blazor**: A framework for building interactive web UIs using C# and Razor syntax instead of JavaScript. - **Native Platforms**: The ability to run these web UIs within native applications.  In a Blazor Hybrid app, Blazor components run on .NET in the native application process (not WebAssembly), while the UI is rendered to an embedded WebView control. This approach allows developers to:  - Reuse web UI components in native applications. - Access native platform features directly from C# code. - Share business logic across web and native platforms. - Leverage the rich Blazor component ecosystem.  This model enables a single C# codebase to target multiple platforms, reducing development time and cost while improving maintainability [[1]](https://www.avidclan.com/blog/blazor-hybrid-with-dot-net-maui-build-cross-platform-web-and-native-apps-in-2025/).  ## Setting Up a .NET MAUI Blazor Hybrid Project  To get started, ensure you have the latest .NET SDK (8 or higher) and Visual Studio 2025 with the MAUI workload installed.  Create a new .NET MAUI Blazor Hybrid application:  ```bash dotnet new maui-blazor -n MauiBlazorAI ```  This command scaffolds a project with the following structure:  - **MauiProgram.cs**: The entry point for the application. - **wwwroot/**: Contains web assets like CSS, JavaScript, and images. - **Pages/**: Contains Blazor components. - **Platforms/**: Platform-specific code for iOS, Android, macOS, and Windows.  ### Understanding the Project Structure  The key components of a MAUI Blazor Hybrid application include:  - **BlazorWebView**: A MAUI control that hosts Blazor content. - **MauiApp**: The application host that configures services and initializes the app. - **Shared Razor Components**: UI components that can be reused across platforms.  Example `MainPage.xaml`:  ```xml                                              ```  ## Building Rich UIs with Blazor Components in MAUI  One of the key advantages of using Blazor Hybrid in MAUI is the ability to leverage the rich ecosystem of Blazor components and libraries.  ### Adding Blazor Component Libraries  Add a reference to a popular Blazor component library like MudBlazor:  ```bash dotnet add package MudBlazor ```  Configure the library in `MauiProgram.cs`:  ```csharp using MudBlazor.Services;  public static MauiApp CreateMauiApp() {     var builder = MauiApp.CreateBuilder();     builder         .UseMauiApp()         .ConfigureFonts(fonts =>         {             fonts.AddFont(\"OpenSans-Regular.ttf\", \"OpenSansRegular\");         });      builder.Services.AddMauiBlazorWebView();     builder.Services.AddMudServices();      return builder.Build(); } ```  ### Creating a Rich Dashboard UI  Example dashboard page using MudBlazor components:  ```razor @page \"/dashboard\" @using MudBlazor       Smart Home Dashboard                                            Temperature                 72°F                 Living Room                                              Recent Activity                           Front door opened at 8:30 AM                          @code {     private bool lightsOn = true; } ```  ## Bridging Native and Web: Accessing Native Features from Blazor  Blazor Hybrid in MAUI allows direct access to native platform features from your Blazor components.  ### Creating a Native Service  Example service for device features:  ```csharp public interface IDeviceService {     Task GetDeviceInfo();     Task TakePicture(string fileName);     Task GetCurrentLocation(); }  public class DeviceService : IDeviceService {     // Implementation as in the original article } ```  Register the service in `MauiProgra"},{"slug":"csharp-dictionaries","title":"Dictionaries in CSharp","description":"Learn about dictionaries in C# and how to use them effectively.","tags":["Csharp","Dictionaries","Hashmaps"],"pubDate":"2023-05-01","content":" **C# provides a powerful data structure called Dictionary that allows you to store key-value pairs.**  ## Introduction  A dictionary is a collection of key-value pairs. It is a data structure used to store data in the form of key-value pairs. The key-value pair is also referred to as an entry. The key is used to retrieve the data associated with it. The key must be unique and immutable. The value can be changed. The value can be of any type. The key and value can be of the same type or of different types.  ## Dictionary Characteristics  - `````` stores key-value pairs. - Comes under .Collections.Generic namespace. - Implements `````` interface. - Keys must be unique and cannot be null. - Values can be null or duplicate. - Values can be accessed bypassing associated key in the indexer e.g. `myDictionary[key]`. - Elements are stored as `KeyValuePair` objects.  ## Creating a Dictionary  You can create the ``````  object bypassing the type of keys and values it can store. The following example shows how to create a dictionary and add key-value pairs.  ```csharp using System; using System.Collections.Generic;  namespace Dictionary {     class Program     {         static void Main(string[] args)         {             Dictionary capitals = new Dictionary();             capitals.Add(\"England\", \"London\");             capitals.Add(\"Germany\", \"Berlin\");             capitals.Add(\"Russia\", \"Moscow\");             capitals.Add(\"USA\", \"Washington\");             capitals.Add(\"Ukraine\", \"Kyiv\");              foreach (KeyValuePair keyValue in capitals)             {                 Console.WriteLine(keyValue.Key + \" - \" + keyValue.Value);             }              Console.WriteLine(\"The capital of England is \" + capitals[\"England\"]);             Console.WriteLine(\"The capital of Germany is \" + capitals[\"Germany\"]);             Console.WriteLine(\"The capital of Russia is \" + capitals[\"Russia\"]);             Console.WriteLine(\"The capital of USA is \" + capitals[\"USA\"]);             Console.WriteLine(\"The capital of Ukraine is \" + capitals[\"Ukraine\"]);              capitals.Remove(\"USA\");              Console.WriteLine(\"The capital of USA is \" + capitals[\"USA\"]);         }     } } ```  ## Dictionary Class Hierarchy  ![Dictionary Class Hierarchy](https://www.tutorialsteacher.com/Content/images/csharp/generic-dictionary.png)  ## Dictionary Methods  The `````` class provides various methods to perform different operations on the dictionary. The following table lists some of the commonly used methods of the `````` class.  | Method                        | Description                                                           | |-------------------------------|-----------------------------------------------------------------------| | Add(TKey, TValue)             | Adds an element with the specified key and value into the dictionary. | | Clear()                       | Removes all the elements from the dictionary.                         | | ContainsKey(TKey)             | Checks whether the specified key exists in the dictionary.            | | ContainsValue(TValue)         | Checks whether the specified value exists in the dictionary.          | | Remove(TKey)                  | Removes the element with the specified key from the dictionary.       | | TryGetValue(TKey, out TValue) | Gets the value associated with the specified key.                     | | Count                         | Gets the number of elements in the dictionary.                        |  ## Dictionary Properties  The `````` class provides various properties to get information about the dictionary. The following table lists some of the commonly used properties of the `````` class.  | Property | Description                                                                 | |----------|-----------------------------------------------------------------------------| | Comparer | Gets the `````` that is used to determine equality of keys for the dictionary. | | Keys     | Gets a collection containing the keys in the dictionary.                    | | Values   | Gets a collection containing the values in the dictionary.                  |  ## Immutable Dictionaries  In addition to the standard Dictionary, C# also provides Immutable Dictionaries. These are dictionaries that cannot be modified after they are created, ensuring thread-safety and preventing unintended modifications.  ### Creating an Immutable Dictionary  To use Immutable Dictionaries, you need to include the `System.Collections.Immutable` namespace. Here's an example of how to create an Immutable Dictionary:  ```csharp using System.Collections.Immutable;  var immutableDictionary = ImmutableDictionary.Empty     .Add(\"One\", 1)     .Add(\"Two\", 2)     .Add(\"Three\", 3); ```  ### Characteristics of Immutable Dictionaries  - Once created, the dictionary cannot be modified. - Any operation that would modify the dictionary (like Add or Remove) returns a new ImmutableDictionary instance. - Provides thread-safe read "},{"slug":"difference-between-parallel-foreach-and-parallel-foreachasync-in-csharp","title":"Difference Between Parallel.ForEach and Parallel.ForEachAsync in C#","description":"An in-depth comparison of Parallel.ForEach and Parallel.ForEachAsync in C# for efficient parallel processing.","tags":["C#","Parallel Programming","Asynchronous","Programming"],"pubDate":"2024-12-08","content":" **Understanding the differences between Parallel.ForEach and Parallel.ForEachAsync is crucial for efficient parallel processing in C#.**  ## Introduction  C# provides powerful constructs for parallel and asynchronous programming, allowing developers to write efficient and scalable applications. Two such constructs are `Parallel.ForEach` and `Parallel.ForEachAsync`. This article explores the differences between these two methods, helping you choose the right one for your use case.  ## Parallel.ForEach  `Parallel.ForEach` is part of the `System.Threading.Tasks` namespace and is used for parallel processing of collections. It divides the work among multiple threads, utilizing multiple cores for faster execution.  ### Key Features  - **Synchronous Execution**: Executes tasks synchronously across multiple threads. - **Blocking Call**: The method blocks the calling thread until all iterations are complete. - **Ideal for CPU-bound Operations**: Best suited for operations that are CPU-intensive and do not involve I/O operations.  #### Example  ```csharp using System; using System.Collections.Generic; using System.Threading.Tasks;  class Program {     static void Main()     {         var numbers = new List { 1, 2, 3, 4, 5 };         Parallel.ForEach(numbers, number =>         {             Console.WriteLine($\"Processing number: {number}\");         });     } } ```  ### Real-World Scenario  Consider a scenario where you need to perform complex mathematical calculations on a large dataset. Using `Parallel.ForEach`, you can distribute these calculations across multiple threads, significantly reducing the time required to process the entire dataset.  ### Performance Considerations  When using `Parallel.ForEach`, it's important to consider the overhead of thread management. While it can significantly speed up CPU-bound tasks, the benefits may diminish if the task is not sufficiently parallelizable or if the overhead of managing threads outweighs the performance gains.  ### Advanced Usage  In more advanced scenarios, `Parallel.ForEach` can be combined with other parallel constructs to achieve even greater performance improvements. For example, you can use it in conjunction with `PLINQ` (Parallel LINQ) to perform complex data transformations in parallel.  ## Parallel.ForEachAsync  `Parallel.ForEachAsync` is a newer addition to the .NET ecosystem, introduced to handle asynchronous operations more efficiently.  ### Key Features  - **Asynchronous Execution**: Executes tasks asynchronously, allowing for non-blocking operations. - **Non-blocking Call**: The method does not block the calling thread, making it suitable for I/O-bound operations. - **Ideal for I/O-bound Operations**: Best suited for operations that involve I/O, such as network calls or file access.  #### Example  ```csharp using System; using System.Collections.Generic; using System.Threading.Tasks;  class Program {     static async Task Main()     {         var numbers = new List { 1, 2, 3, 4, 5 };         await Parallel.ForEachAsync(numbers, async (number, cancellationToken) =>         {             await Task.Delay(1000); // Simulate asynchronous work             Console.WriteLine($\"Processing number: {number}\");         });     } } ```  ### Real-World Scenario  Imagine you are developing an application that needs to fetch data from multiple web APIs. Using `Parallel.ForEachAsync`, you can initiate multiple asynchronous requests simultaneously, improving the overall responsiveness of your application.  ### Performance Considerations  `Parallel.ForEachAsync` is particularly beneficial for I/O-bound tasks where the main bottleneck is waiting for external resources. By not blocking the calling thread, it allows other operations to proceed, improving the overall throughput of the application.  ### Advanced Usage  For advanced use cases, `Parallel.ForEachAsync` can be integrated with other asynchronous patterns, such as `async/await`, to create highly responsive applications that can handle a large number of concurrent operations.  ## Key Differences  - **Execution Model**: `Parallel.ForEach` is synchronous, while `Parallel.ForEachAsync` is asynchronous. - **Use Cases**: Use `Parallel.ForEach` for CPU-bound tasks and `Parallel.ForEachAsync` for I/O-bound tasks. - **Blocking Behavior**: `Parallel.ForEach` blocks the calling thread, whereas `Parallel.ForEachAsync` does not.  ## Comparison Table  | Feature                      | Parallel.ForEach          | Parallel.ForEachAsync       | |------------------------------|---------------------------|-----------------------------| | Nature                       | Synchronous               | Asynchronous                | | Support for async/await      | No                        | Yes                         | | Blocking                     | Blocks the calling thread until complete | Non-blocking; returns a Task | | Best for                     | CPU-bound tasks           | IO-bound or asynchronous tasks | | Cancellation     "},{"slug":"docker-introduction-and-best-practices","title":"Docker: Introduction, Concepts, and Best Practices","description":"A practical guide to Docker, covering its core concepts, use cases, and best practices for developers.","tags":["docker","containers","devops"],"pubDate":"2025-05-12","content":" # Docker: Introduction, Concepts, and Best Practices  Docker has revolutionized the way developers build, ship, and run applications. By using containers, Docker enables consistent environments, scalability, and simplified deployment processes.  > **Containers vs. Virtual Machines: A Simple Analogy** > - Think of a virtual machine (VM) as a whole house with its own kitchen, bathroom, and utilities. Each VM runs a full operating system, which makes it heavy and slow to start. > - A Docker container is like a studio apartment in a large building. All apartments share the same utilities (host OS), but each has its own space and locks. Containers are lightweight, fast, and use fewer resources.  **Who is this guide for?** This guide is perfect for developers, DevOps engineers, students, and anyone new to containerization. You'll learn fundamental concepts, hands-on examples, and best practices to get started quickly—even if you've never used Docker before.  **Prerequisites** - A computer running macOS, Windows, or Linux - Basic familiarity with the command line (but you can copy-paste commands) - Administrative privileges to install Docker  ## What is Docker?  Docker is an open-source platform that automates the deployment, scaling, and management of applications using containerization. Containers are lightweight, portable, and ensure that your application runs the same regardless of the environment.  > **Real-world analogy:** > Imagine you want to ship a cake to a friend in another city. If you just hand over the cake, it might get ruined during transport. But if you put it in a sturdy, sealed box (the container), you can be sure it arrives in perfect condition, no matter the journey. Docker containers 'box up' your application and its dependencies so it always works as intended.  ## Why Use Docker?  - **Consistency:** Eliminate \"works on my machine\" problems - **Isolation:** Run multiple applications on the same host without conflicts - **Portability:** Move containers across environments (local, staging, production) - **Efficiency:** Lightweight compared to virtual machines  ## Core Docker Concepts  ### Images A Docker image is a read-only template with instructions for creating a container. Images are built from Dockerfiles and can be shared via registries like Docker Hub.  Think of an image as a recipe or blueprint: it describes exactly what goes into your container so you can recreate identical environments every time.  ### Containers A container is a runnable instance of an image. Containers are isolated from each other and the host system, but can communicate through defined channels.  Containers are like dishes prepared from your recipe: they package and isolate your application and its dependencies in a portable environment.  ### Dockerfile A Dockerfile is a text file that contains instructions to build a Docker image. Example:  ```Dockerfile FROM node:18 WORKDIR /app COPY package*.json ./ RUN npm install COPY . . CMD [\"npm\", \"start\"] ```  ### Volumes Volumes are used to persist data generated by and used by Docker containers. Volumes let you store important files on your computer, even after the container is deleted.  **Why use volumes?** - Store a database or uploaded files outside the container - Share code between your computer and the container for live development  **Example: Mount a host directory into the container:** ```bash docker run -v ~/data:/data myapp ``` This command makes your `~/data` folder available inside the container at `/data`.  **Named volumes:** ```bash docker volume create myvolume docker run -v myvolume:/app/data myapp ```  ### Networks Docker networks allow containers to communicate with each other and with external systems. This is helpful for connecting a web app to a database, for example.  **Example: Create a network and run containers on it:** ```bash docker network create my-network docker run --network my-network --name db mydb docker run --network my-network --name app myapp ```  **Inspect networks:** ```bash docker network ls docker network inspect my-network ```  **Tip:** By default, all containers are attached to the `bridge` network. Custom networks make communication easier and more secure.  ## Getting Started with Docker (Step-by-Step)  ### 1. Install Docker - Download and install Docker Desktop from [docker.com](https://www.docker.com/products/docker-desktop/). - Follow the instructions for your OS. - After installation, open a terminal and run: ```bash docker --version ``` You should see the Docker version info.  ### 2. Run Your First Container ```bash docker run hello-world ``` This downloads a test image and prints a welcome message.  ### 3. Basic Docker Commands  - `docker build -t myapp .` — Build an image from a Dockerfile - `docker run -d -p 3000:3000 myapp` — Run a container in detached mode, mapping port 3000 - `docker ps` — List running containers - `docker stop ` — Stop a running container - `docker rm ` — Remove a container - `docker rmi ` — Remo"},{"slug":"entity-framework-with-compiled-query","title":"Pros and Cons of Entity Framework's Compiled Query","description":"Explore the advantages and disadvantages of using compiled queries in Entity Framework with practical examples in C# and .NET 7.","tags":["Entity Framework","C#",".Net","Performance"],"pubDate":"2023-05-17","content":" **Entity Framework (EF) is an object-relational mapper (ORM) that allows developers to interact with databases using objects. EF provides a number of features that make it a powerful tool for data access, including compiled queries.**  Compiled queries are pre-processed by EF and stored in memory. This can improve performance by reducing the amount of time that EF needs to spend parsing and executing the query each time it is used.  There are a number of pros and cons to using compiled queries in EF.  ## Pros:  - Compiled queries can improve performance by reducing the amount of time that EF needs to spend parsing and executing the query each time it is used. - Compiled queries can be reused, which can further improve performance. - Compiled queries can be cached, which can further improve performance.  ## Cons:  - Compiled queries can be more difficult to debug than ad-hoc queries. - Compiled queries can be less flexible than ad-hoc queries. - Compiled queries can be more difficult to maintain than ad-hoc queries.  ## Examples with C# and .NET 7  Here is an example of how to use compiled queries in EF with C# and .NET 7:  ```csharp using System; using System.Data.Entity;  namespace Example {     class Program     {         static void Main(string[] args)         {             // Create a new context.             var context = new MyContext();              // Create a new query.             var query = context.Set().Where(e => e.Name == \"John Doe\");              // Compile the query.             var compiledQuery = query.Compile();              // Execute the query.             var results = compiledQuery.ToList();              // Print the results.             foreach (var result in results)             {                 Console.WriteLine(result.Name);             }         }     }      public class MyContext : DbContext     {         public DbSet MyEntities { get; set; }          protected override void OnModelCreating(DbModelBuilder modelBuilder)         {             modelBuilder.Entity()                 .Property(e => e.Name)                 .IsRequired();         }     }      public class MyEntity     {         public int Id { get; set; }         public string Name { get; set; }     } } ```  This code will compile the query and then execute it. The results of the query will be printed to the console.  ### Reusing a Compiled Query:  ```csharp // Define the compiled query outside the method. static Func compiledQuery = EF.CompileQuery(     (MyContext context, string name) => context.Set().FirstOrDefault(e => e.Name == name) );  // Use the compiled query multiple times. using (var context = new MyContext()) {     var result1 = compiledQuery(context, \"John Doe\");     var result2 = compiledQuery(context, \"Jane Smith\");     var result3 = compiledQuery(context, \"Mike Johnson\"); } ```  In this example, the compiled query is defined once and can be reused multiple times within the same context, which can provide a performance benefit.  ### Caching a Compiled Query:  ```csharp // Define the compiled query with caching enabled. static Func cachedQuery = EF.CompileQuery(     (MyContext context, string name) => context.Set().FirstOrDefault(e => e.Name == name),     cacheKey: \"myCachedQuery\" );  // Use the cached query. using (var context = new MyContext()) {     var result1 = cachedQuery(context, \"John Doe\");     var result2 = cachedQuery(context, \"Jane Smith\"); } ```  By specifying a cache key, the compiled query can be stored in memory for subsequent usage, providing an additional performance boost.  ### Parameterized Compiled Query:  ```csharp // Define a parameterized compiled query. static Func> parameterizedQuery = EF.CompileQuery(     (MyContext context, int count) => context.Set().Take(count) );  // Use the parameterized query. using (var context = new MyContext()) {     var results1 = parameterizedQuery(context, 5);    // Get the first 5 entities.     var results2 = parameterizedQuery(context, 10);   // Get the first 10 entities. } ```  In this example, the compiled query takes a parameter (count) that allows you to customize the query based on your requirements.  It's important to note that the benefits and drawbacks mentioned in the article still apply to these examples, and developers should carefully consider the trade-offs before deciding to use compiled queries in their Entity Framework applications.  Compiled queries can be a powerful tool for improving the performance of EF applications. However, it is important to weigh the pros and cons of using compiled queries before deciding whether or not to use them.  ## Conclusion  In this article, we learned about the pros and cons of using compiled queries in Entity Framework. We also explored practical examples of how to implement compiled queries in C# and .NET 7.  ## References  * [Asynchronous programming with async and await](https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/concepts/async/) * [Task-based Asynchronous Pattern](https:"},{"slug":"everything-you-need-to-know-on-latest-features-of-react-19","title":"Everything You Need to Know on Latest Features of React 19","description":"Explore the latest features introduced in React 19 and how they enhance the development experience.","tags":["React","Javascript","Web Development","Frontend"],"pubDate":"2024-11-10","content":" React 19 introduces a host of new features and improvements that aim to enhance the developer experience and performance of React applications. In this article, we will explore these features in detail and understand how they can be leveraged in your projects.  React 19 introduces a slew of new features aimed at simplifying web development and improving performance. Here's a quick rundown of what's new: - **The React Compiler**: Converts React code into regular JavaScript, potentially doubling performance. - **Actions**: Simplify handling data and interactions within web pages. - **Server Components**: Render on the server for faster page loads and better SEO. - **Asset Loading**: Background loading of assets for smoother page transitions. - **Document Metadata**: Easy SEO enhancements with the `` component. - **Web Components**: Improved compatibility for more flexible development. - **Enhanced Hooks**: More control over component lifecycle and state.  React 19 makes web development faster, more efficient, and less complicated, from coding to deployment. Upgrading involves assessing your current app, implementing changes gradually, leveraging tools like codemods, testing thoroughly, and using Strict Mode to ensure compatibility. The effort pays off with better performance and easier maintenance.  ### Key Features of React 19  React 19 is here, and it's packed with new stuff that makes building websites easier and faster. Let's dive into the big updates and see how they can help you in your projects.  #### The React Compiler  First up, we've got a compiler. This tool changes React code into regular JavaScript, making things run up to twice as fast. It's like giving your car a turbo boost but for your code. Right now, you can choose to use it, but it'll become the standard way React works in the future.  #### Actions  Actions are a new way to deal with things like forms on your website. They let you update your page's info when someone fills out a form, all without making it too complicated. It's a big help for keeping things simple.  #### Server Components  These components do their job on the server before sending the finished page to the user. This means your website loads quicker, is better for search engines, and handles data more smoothly.  #### Asset Loading  React 19 makes it easier to get your pictures and other files ready faster. It starts loading them in the background while people are still looking at the current page. This means less waiting time when moving to a new page.  #### Document Metadata  Now, adding things like titles and meta tags to your pages is straightforward with a new component called ``. This helps with SEO and making sure your brand looks right across your site, without having to repeat the same code everywhere.  #### Web Components  React now works better with Web Components, which means you can mix and match parts of your website more easily. It's great for using React in places where you weren't able to before.  #### Enhanced Hooks  Hooks are better than ever, giving you more control over when your code runs and updates. This keeps your website running smoothly and makes writing code a bit easier. Overall, React 19 brings a lot of improvements that make building websites with React more efficient and less of a headache. From faster code with the new compiler to easier ways to handle forms and load content, there's a lot to be excited about.  ### React 19 in Action  React 19 is showing us some pretty cool stuff that developers can use to make websites faster, handle more users, and just work better overall. Here are a few examples of how its new features are making a real difference:  - **Faster Time-to-Interactive with Concurrent Rendering**: A big online store switched to React 19 and used its new way of rendering pages to make their site 42% quicker to use. This means pages are ready to interact with faster, keeping visitors happy and sticking around longer. - **Improved SEO with Automatic SSR**: An online magazine started using Server Components, which let their pages be prepared on the server first. This made it easier for search engines to understand their content, boosting their spot in search results by 19% in two months. - **Smoother UX with Suspense**: A tech company tried out Suspense, a feature that lets you show something on the screen while waiting for the rest of the data to load. This got rid of annoying loading icons, making the site feel smoother when moving from page to page. - **Easier Global State Management**: A new financial tech company used React 19's updated tools for managing data across the whole app. They managed to cut down on unnecessary code by 62% and got new features out the door 47% faster. This is all thanks to React 19 making it easier to handle data.  As these stories show, React 19 is helping developers make better websites faster. Whether it's making the site faster, easier to find on Google, nicer to use, or simpler to code for, Rea"},{"slug":"factory-design-pattern-with-csharp","title":"Factory Design Pattern With C#","description":"Learn about the Factory Design Pattern in C# with examples.","tags":["Csharp","Design Patterns","Factory Pattern"],"pubDate":"2023-04-29","content":" **The factory design pattern is a creational pattern that provides a way to create objects without specifying the exact class of object that will be created.**  In a car factory, it could be used to create different types of cars (e.g. sedans, SUVs, trucks) without specifying the exact make and model of each car.  ## When to use it?  - When you don’t know ahead of time what class object you need. - When all the potential classes are in the same subclass hierarchy. - To centralize class selection code. - When you don’t want the user to have to know every subclass.  ## Example  Here’s an example of how the factory design pattern could be implemented in C# for a car factory:  ```csharp abstract class Car {     public abstract string GetType(); }  class Sedan : Car {     public override string GetType() {         return \"Sedan\";     } }  class SUV : Car {     public override string GetType() {         return \"SUV\";     } }  class Truck : Car {     public override string GetType() {         return \"Truck\";     } }  class CarFactory {     public static Car GetCar(string carType) {         switch (carType) {             case \"Sedan\":                 return new Sedan();             case \"SUV\":                 return new SUV();             case \"Truck\":                 return new Truck();             default:                 return null;         }     } }  class Program {     static void Main(string[] args) {         Car sedan = CarFactory.GetCar(\"Sedan\");         Console.WriteLine(sedan.GetType()); // Output: \"Sedan\"     } } ```  In this example, the `CarFactory` class is the factory and the `Sedan`, `SUV`, and `Truck` classes are the products. The `GetCar` method of the `CarFactory` class is responsible for creating the appropriate type of car based on the input car type. The factory pattern has encapsulated the object creation logic, making it easy to add new products or change the existing product classes without affecting the client code.  You can use this example as a base and expand it by adding properties or methods to the Car class and its derived classes, like number of doors, engine size, and so on. --- "},{"slug":"getting-started-with-nextjs","title":"Getting Started with Next.js","description":"Learn how to build modern web applications with Next.js, the React framework for production.","tags":["Nextjs","React","Tutorial"],"pubDate":"2023-05-15","content":" Next.js is a powerful React framework that enables you to build fast, SEO-friendly web applications with server-side rendering, static site generation, and more.  ## What is Next.js?  Next.js is a React framework developed by Vercel that provides a set of features for building modern web applications:  - **Server-Side Rendering (SSR)**: Renders pages on the server for better SEO and initial load performance - **Static Site Generation (SSG)**: Pre-renders pages at build time for even faster performance - **API Routes**: Create API endpoints as part of your Next.js application - **File-based Routing**: Create routes based on the file structure in your pages directory - **Built-in CSS Support**: Import CSS files directly in your components - **Image Optimization**: Automatically optimize images for better performance  ## Setting Up a Next.js Project  Getting started with Next.js is straightforward. Here's how you can create a new project:  ```bash npx create-next-app@latest my-next-app cd my-next-app npm run dev ```  This will create a new Next.js project and start the development server at `http://localhost:3000`.  ## File Structure  A typical Next.js project has the following structure:  ``` my-next-app/   ├── node_modules/   ├── public/   ├── src/   │   ├── app/   │   │   ├── page.tsx   │   │   └── layout.tsx   │   ├── components/   │   └── styles/   ├── .gitignore   ├── next.config.js   ├── package.json   └── tsconfig.json ```  ## Creating Pages  With the App Router in Next.js, you create pages by adding files to the `app` directory. For example, to create a page at `/about`, you would create a file at `app/about/page.tsx`:  ```jsx export default function AboutPage() {   return (            About Us       This is the about page of our website.        ); } ```  ## Data Fetching  Next.js provides several ways to fetch data for your pages:  ### Server Components (Default in App Router)  ```jsx // This component will be rendered on the server export default async function Page() {   const data = await fetch('https://api.example.com/data');   const jsonData = await data.json();    return (            {jsonData.title}       {jsonData.description}        ); } ```  ## Conclusion  Next.js is a powerful framework that makes building React applications easier and more efficient. With its built-in features for server-side rendering, static site generation, and more, it's an excellent choice for building modern web applications.  In future posts, we'll explore more advanced features of Next.js, such as dynamic routes, API routes, and deployment strategies.  Happy coding! "},{"slug":"google-9-hour-ai-prompt-engineering-course-in-10-minutes","title":"Google's 9-Hour AI Prompt Engineering Course in 10 Minutes","description":"A concise summary of Google's comprehensive AI Prompt Engineering Course, distilled into a 10-minute read.","tags":["Ai","Prompt Engineering","Google","Programming"],"pubDate":"2024-12-10","content":" The landscape of artificial intelligence is constantly evolving, and as these advancements permeate our daily lives, mastering the art of AI interaction becomes crucial. Google's 9-hour AI Prompt Engineering Course is a comprehensive guide designed to equip you with the skills necessary to communicate effectively with AI systems. This article aims to distill the core teachings of the course into a digestible format, providing a detailed exploration of key concepts and techniques to enhance your AI interactions.  ## Introduction: A Cliffnotes Version  I took Google's prompt engineering course so that you don’t have to invest a full nine hours. While I've summarized the course content for you, it’s not enough just to read through this article. To help you retain this information, I've included an assessment at the end. Research shows that revisiting information right after learning it significantly enhances retention. So, let’s dive into the course structure, which is divided into four modules:  1. **Module 1: Start Writing Prompts Like a Pro** – This introduces frameworks for crafting effective prompts. 2. **Module 2: Design Prompts for Everyday Work Tasks** – Covers prompts for emails, brainstorming, building tables, and summarizing documents. 3. **Module 3: Using AI for Data Analysis and Presentations** – Focuses on practical applications in data handling and presentation building. 4. **Module 4: Use AI as a Creative or Expert Partner** – Discusses advanced prompting techniques such as prompt chaining, chain of thought, and tree of thought, and provides a framework for creating AI agents.  ## Module 1: Prompting Essentials  The foundation of prompt engineering is the process of providing specific instructions to a generative AI tool to achieve a desired outcome. This could involve text, images, video, sound, or even code. Google’s course offers a five-step framework for designing effective prompts:  - **Task**: Clearly define what you want the AI to do. For instance, if your friend’s birthday is approaching and they love anime, your task might be to suggest an anime-related gift. - **Context**: The more context you provide, the better the output. For example, specify your friend’s age, favorite anime, and past gifts they’ve enjoyed. - **References**: Provide examples or templates to guide the AI. This helps clarify your expectations and enables the AI to produce more accurate results. - **Evaluate**: Once you receive the output, assess its quality. Does it meet your expectations? - **Iterate**: If the output isn’t quite right, refine and adjust your prompts. This iterative process is crucial for improving results.  A mnemonic to remember this framework is \"Tiny Crabs Ride Enormous Iguanas.\" While whimsical, it can help solidify the steps in your memory. The course also emphasizes the need for iteration, highlighting four methods:  - **Revisit the Framework**: Add more context, references, or a persona. - **Separate into Shorter Sentences**: Simplify complex prompts. - **Try Different Phrasing or Analogous Tasks**: Rephrase requests or suggest related tasks. - **Introduce Constraints**: Narrow the focus for more specific outputs.  ### Multimodal Prompting  With AI systems capable of handling multiple modalities, such as text, images, audio, and video, you might need to specify the input and output formats explicitly. This doesn’t alter the basic prompting principles but requires attention to detail regarding the types of data involved.  ### Hallucinations and Biases  AI tools can sometimes produce outputs that are incorrect or nonsensical, known as \"hallucinations,\" and may reflect biases present in their training data. To mitigate these issues, the course recommends a \"human-in-the-loop\" approach, ensuring that outputs are verified and checked for accuracy.  ## Module 2: Design Prompts for Everyday Work Tasks  In this module, Google’s course delves into using AI to streamline everyday work tasks. By leveraging the five-step framework, you can enhance productivity and creativity in various scenarios.  ### Writing Emails and Other Content  One of the most common uses of generative AI tools is content creation. Whether it's writing emails or drafting articles, AI can significantly reduce the time and effort required. Here are some practical examples:  - **Email Communication**: When drafting emails, specificity is key. Instead of requesting a \"casual\" tone, specify your intent with phrases like \"friendly, easy-to-understand tone, like explaining to a curious friend.\" For instance, if you’re a gym manager announcing a schedule change, you might say: \"Write an email informing our staff of the new gym schedule. Highlight the change in the Cardio Blast class from 7:00 a.m. to 6:00 a.m. Make it professional, friendly, and concise.\" - **Content Creation**: For more substantial writing tasks, such as essays or newsletters, providing context and tone references can help the AI match your style. This module suggests "},{"slug":"graph-in-csharp","title":"Graphs in C#","description":"An introduction to implementing graphs in C# with a simple example","tags":["C#","Graphs","Data Structures","Algorithms","Programming"],"pubDate":"2023-12-10","content":" In data structures and algorithms, a graph is a powerful and versatile data structure that consists of a set of nodes (also called vertices) and a set of edges connecting these nodes. Each edge has a direction and a weight, and may represent a relationship or a flow of information between the nodes it connects. Graphs are used to represent various types of networks and relationships, making them essential in many areas of computer science and software development.  ## 1. Understanding Graphs  ### 1.1 Definition  A graph is composed of:  - Nodes (vertices): Represent entities or points in the graph - Edges: Connect nodes and may have direction and weight  ### 1.2 Applications  Graphs are widely used to model:  - Social networks - Transportation networks - Communication networks - Many other real-world problems  Graphs can be used to solve various problems, such as finding the shortest path between two nodes or determining whether a graph is connected.  ## 2. Implementing a Graph in C#  Let's look at a simple implementation of a graph class in C#:  ```csharp // Graph class public class Graph {     // Dictionary to store the edges in the graph     Dictionary>> edges = new Dictionary>>();      // Method to add an edge to the graph     public void AddEdge(int u, int v, int w)     {         if (!edges.ContainsKey(u))             edges.Add(u, new List>());          edges[u].Add(new Tuple(v, w));     }      // Method to get the neighbors of a node     public List> GetNeighbors(int u)     {         return edges[u];     } } ```  This Graph class uses a dictionary to store the edges in the graph, with the keys representing the starting node of the edge and the values representing the ending nodes of the edge. Each edge is represented as a tuple containing the ending node and the weight of the edge. The AddEdge method adds an edge to the graph, and the GetNeighbors method returns the list of neighbors for a given node.  [GitHub Source Code](https://github.com/nirzaf/GraphDataStructure) "},{"slug":"how-to-enable-multi-factor-authentication-using-microsoft-authenticator","title":"How to Enable Multi-Factor Authentication using Microsoft Authenticator","description":"Learn how to enable multi-factor authentication using Microsoft Authenticator for your Office 365 account.","tags":["O365","Mfa","2fa","Authenticator","Security","Microsoft"],"pubDate":"2024-10-06","content":" What is Multi-Factor Authentication?  Multi-Factor Authentication (MFA) adds an extra security layer to your Office 365 account by requiring two forms of verification: 1. Something you know (your password) 2. Something you have (your phone with the Authenticator app)  This prevents unauthorized access even if your password is compromised. Microsoft reports that MFA blocks over 99.9% of account compromise attempts.  ### What You'll Need - Office 365 account - Smartphone (iOS or Android) - Internet connection - 5-10 minutes  ### Setup Process Overview 1. Download Microsoft Authenticator app 2. Configure your Office 365 account 3. Link your account by scanning a QR code 4. Add a backup verification method  ### Step 1: Download Microsoft Authenticator App  **For iPhone:** - Open App Store and search for \"Microsoft Authenticator\" - Download and install the app from Microsoft Corporation - Open the app and accept terms of use - Allow notifications when prompted  **For Android:** - Open Google Play Store and search for \"Microsoft Authenticator\" - Download and install the app from Microsoft Corporation - Open the app and accept terms of use - Allow notifications when prompted               Microsoft Authenticator on iOS                Microsoft Authenticator on Android      > **Tip**: Make sure to place the Microsoft Authenticator app on your home screen for easy access. You'll be using it frequently when signing in to your accounts.  ### Step 2: Configure Office 365 Account  **On your computer:**  1. Go to [portal.office.com](https://portal.office.com) and sign in  2. Access MFA settings:    - If prompted with \"More information required\" or \"Set up your account\", click \"Next\" or \"Set it up now\"    - Otherwise, go directly to [https://aka.ms/mfasetup](https://aka.ms/mfasetup)  3. On the Security info page:    - Click \"+ Add sign-in method\"    - Select \"Authenticator app\" from the dropdown    - Choose \"Receive notifications for verification\"    - Click \"Set up\"    - A QR code will appear — keep this window open  ### Step 3: Link Your Account  **On your phone:**  1. Open the Microsoft Authenticator app 2. Tap the + icon in the top-right corner 3. Select \"Work or school account\" 4. Allow camera access if prompted 5. Scan the QR code displayed on your computer screen  ![Scan the QR code](https://ik.imagekit.io/fazrinphcc/MultiFactorAuthentication/page2.jpg?updatedAt=1717915666195) ![QR code scanning](https://ik.imagekit.io/fazrinphcc/MultiFactorAuthentication/page3.jpg?updatedAt=1717915666202)  **Complete setup:**  1. After scanning, your account will appear in the app 2. Return to your computer and click \"Next\" 3. A test notification will be sent to your phone 4. Tap \"Approve\" on your phone 5. On your computer, you'll see \"Your configuration was successful\" 6. Click \"Next\"  ![Successful configuration](https://ik.imagekit.io/fazrinphcc/MultiFactorAuthentication/page6.jpg?updatedAt=1717915666145)  > **Tip**: If scanning fails, look for a manual code entry option on the setup screen  ### Step 4: Add a Backup Method  Adding a backup method is critical in case you lose access to your phone or the Authenticator app.  1. When prompted, select \"Phone\" as your backup method 2. Enter your mobile number with country code (e.g., +1 for USA) 3. Choose \"Text message\" for verification (recommended) 4. Click \"Next\"  ![Add phone number](https://ik.imagekit.io/fazrinphcc/MultiFactorAuthentication/page4.jpg?updatedAt=1717915666241)  5. You'll receive a verification code via text message 6. Enter this code on your computer and click \"Verify\" 7. Click \"Done\" to complete setup  ![Verify code](https://ik.imagekit.io/fazrinphcc/MultiFactorAuthentication/page5.jpg?updatedAt=1717915666141)  ### Using MFA After Setup  **Daily Sign-in Process:** 1. Enter your email and password as usual 2. You'll receive either:    - A push notification on your phone (tap Approve)    - A prompt to enter a verification code from the app  **Important Security Tips:** - Only approve sign-ins you initiated yourself - If you receive an unexpected authentication request, deny it and change your password immediately - Keep your phone secure as it's now part of your login security  ### Troubleshooting  - **Lost phone?** Use your backup phone number to sign in - **No notifications?** Check app notification settings and internet connection - **Wrong codes?** Ensure your phone's date/time settings are automatic  ### Conclusion  You've successfully enabled Multi-Factor Authentication for your Office 365 account. Your account is now protected against unauthorized access, even if your password is compromised.  This simple security measure blocks over 99.9% of account compromise attempts, making it one of the most effective ways to protect your digital identity.  If you have any questions or need further assistance, please feel free to contact me or leave a comment below.  ### Video Tutorial  For a visual demonstration of the setup process, check out this helpful video guide:"},{"slug":"how-to-implement-soft-delete-with-entity-framwork-core","title":"Implementing Soft Delete in .NET with Entity Framework Core","description":"Learn how to implement soft delete functionality in your .NET applications using Entity Framework Core, improving data management and recoverability.","tags":["Dotnet","Efcore","Softdelete","Database","Crud"],"pubDate":"2024-10-02","content":" Soft delete is a powerful technique for managing data in your applications. Instead of permanently removing records from your database, soft delete marks them as \"deleted\" while keeping the data intact. This approach offers several benefits, including easy data recovery and improved auditing capabilities. In this article, we'll explore how to implement soft delete in a .NET application using Entity Framework Core.  ## Why Use Soft Delete?  Before diving into the implementation, let's briefly discuss why soft delete is beneficial:  1. **Data Recovery:** Accidentally deleted data can be easily restored. 2. **Auditing:** Maintain a history of when items were \"deleted\". 3. **Consistency:** Preserve referential integrity in related data. 4. **Performance:** In some cases, soft delete can be more performant than hard delete operations.  ## Implementation Steps  Let's walk through the process of implementing soft delete in a .NET application using Entity Framework Core.  ### Step 1: Create the ISoftDeletable Interface  First, we'll define an interface that our entities will implement to support soft delete:  ```csharp public interface ISoftDeletable {     bool IsDeleted { get; set; }     DateTime? DeletedAtUtc { get; set; } } ```  This interface includes two properties:  - `IsDeleted`: A flag indicating whether the entity is considered deleted. - `DeletedAtUtc`: The date and time when the entity was marked as deleted.  ### Step 2: Implement the Interface in Your Entity  Next, implement the `ISoftDeletable` interface in your entity classes. Here's an example with a `Movie` entity:  ```csharp public class Movie : ISoftDeletable {     public int Id { get; set; }     public string Title { get; set; }     public int Year { get; set; }      // ISoftDeletable implementation     public bool IsDeleted { get; set; }     public DateTime? DeletedAtUtc { get; set; } } ```  ### Step 3: Configure the DbContext with Global Query Filter  In your `DbContext` class, override the `OnModelCreating` method to add a global query filter for soft delete. This is a crucial step that eliminates the need to include the `!IsDeleted` condition in every query:  ```csharp public class AppDbContext : DbContext {     public DbSet Movies { get; set; }      protected override void OnModelCreating(ModelBuilder modelBuilder)     {         base.OnModelCreating(modelBuilder);          // Apply global query filter for soft delete         modelBuilder.Model.GetEntityTypes()             .Where(entityType => typeof(ISoftDeletable).IsAssignableFrom(entityType.ClrType))             .ToList()             .ForEach(entityType =>             {                 var parameter = Expression.Parameter(entityType.ClrType, \"e\");                 var property = Expression.Property(parameter, nameof(ISoftDeletable.IsDeleted));                 var falseConstant = Expression.Constant(false);                 var lambdaExpression = Expression.Lambda(Expression.Equal(property, falseConstant), parameter);                  modelBuilder.Entity(entityType.ClrType).HasQueryFilter(lambdaExpression);                    // Add filtered index for better performance (Optional but Recommended)                 modelBuilder.Entity(entityType.ClrType)                     .HasIndex(nameof(ISoftDeletable.IsDeleted))                     .HasFilter($\"\\\"{nameof(ISoftDeletable.IsDeleted)}\\\" = 0\") // Use $\"\" for interpolated strings and escape column name for potential reserved words.                     .HasDatabaseName($\"IX_{entityType.ClrType.Name}_{nameof(ISoftDeletable.IsDeleted)}\");              });     } } ```   This enhanced `OnModelCreating` method now iterates through all entities in your model and applies the global query filter *only* to those that implement the `ISoftDeletable` interface.  It also dynamically creates a filtered index for each of these entities, significantly improving query performance, especially for large datasets.  The code also addresses potential issues with reserved keywords by escaping column names in the filter and index name.    ### Step 4: Implement Soft Delete in Your Service/Repository Layer  This example uses a more robust approach with `ExecuteUpdateAsync` for better performance, especially with large datasets:  ```csharp public async Task SoftDeleteMovieAsync(int id) {     var affectedRows = await _context.Movies         .Where(m => m.Id == id) // No need to check IsDeleted here due to the global filter         .ExecuteUpdateAsync(s => s.SetProperty(m => m.IsDeleted, true)                                    .SetProperty(m => m.DeletedAtUtc, DateTime.UtcNow));      return affectedRows > 0; }   public async Task GetMovieByIdAsync(int id, bool includeDeleted = false) {     var query = _context.Movies.AsQueryable();      if (includeDeleted)     {         query = query.IgnoreQueryFilters();     }      return await query.FirstOrDefaultAsync(m => m.Id == id); }    // Example usage in a controller: [HttpDelete(\"{id}\")] public async Task DeleteMovie(int id) {    "},{"slug":"how-to-use-redis-caching-with-aspnet-core-and-net-8","title":"How to Use Redis Caching With ASP.NET Core and .NET 8","description":"Learn how to use Redis caching with ASP.NET Core and .NET 8 to improve application performance and scalability.","tags":["Redis","Caching","Aspnetcore","Dotnet8","Performance"],"pubDate":"2024-10-01","content":" **Caching is a crucial aspect of web application development. It can help reduce the load on the database, speed up application performance, and improve scalability. Redis, an open-source, in-memory data structure store, is a popular choice for caching in ASP.NET Core applications. In this article, we will explore the steps involved in implementing Redis caching with ASP.NET Core and .NET 8.**  ## Prerequisites  To follow along with this article, you should have the following:  - .NET 8 SDK installed on your machine - Visual Studio 2022 (version 17.8 or later), Visual Studio Code, or JetBrains Rider - Redis server installed locally or access to a remote Redis server  ## Creating a New ASP.NET Core Application  Let's start by creating a new ASP.NET Core application:  1. Open your preferred IDE (Visual Studio, VS Code, or Rider). 2. Create a new ASP.NET Core Web API project. 3. Select \".NET 8.0\" as the target framework. 4. Choose a project name and location, then create the project.  ## Adding Redis Cache  To use Redis caching in your ASP.NET Core application, you need to add the Redis cache provider to your project.  ### Add the Redis cache provider:  Run the following command in the terminal or Package Manager Console:  ```bash dotnet add package Microsoft.Extensions.Caching.StackExchangeRedis ```  ## Configuring Redis Cache  Once you've added the Redis cache provider, configure it in the `Program.cs` file:  ```csharp using Microsoft.Extensions.Caching.StackExchangeRedis;  var builder = WebApplication.CreateBuilder(args);  // Add Redis cache builder.Services.AddStackExchangeRedisCache(options => {     options.Configuration = builder.Configuration.GetConnectionString(\"Redis\");     options.InstanceName = \"SampleInstance\"; });  // Other service registrations...  var app = builder.Build();  // Configure the HTTP request pipeline if (app.Environment.IsDevelopment()) {     app.UseSwagger();     app.UseSwaggerUI(); }  app.UseHttpsRedirection(); app.UseAuthorization(); app.MapControllers();  // Use Redis cache middleware app.UseResponseCaching();  app.Run(); ```  In this code, we're adding the Redis cache provider to the `IServiceCollection` and configuring it with the Redis server address from the configuration. We're also adding the Redis cache middleware to the application pipeline using the `UseResponseCaching` method.  Make sure to add the Redis connection string to your `appsettings.json`:  ```json {   \"ConnectionStrings\": {     \"Redis\": \"localhost:6379\"   },   // Other settings... } ```  ## Using Redis Cache  Now that Redis cache is configured, you can use it in your application to cache data. You can do this by injecting the `IDistributedCache` interface into your controllers or services and using it to read from or write to the cache.  ### Example: Caching Product Data  Here's an example of how to use Redis cache to cache a list of products:  ```csharp using System.Text.Json; using Microsoft.Extensions.Caching.Distributed;  public class ProductService {     private readonly IDistributedCache _cache;     private readonly ApplicationDbContext _dbContext;      public ProductService(IDistributedCache cache, ApplicationDbContext dbContext)     {         _cache = cache;         _dbContext = dbContext;     }      public async Task> GetProductsAsync()     {         var cacheKey = \"ProductList\";         var cachedProducts = await _cache.GetStringAsync(cacheKey);          if (cachedProducts != null)         {             return JsonSerializer.Deserialize>(cachedProducts);         }          var products = await _dbContext.Products.ToListAsync();         var serializedProducts = JsonSerializer.Serialize(products);         var cacheOptions = new DistributedCacheEntryOptions         {             AbsoluteExpirationRelativeToNow = TimeSpan.FromMinutes(30),             SlidingExpiration = TimeSpan.FromMinutes(10)         };          await _cache.SetStringAsync(cacheKey, serializedProducts, cacheOptions);          return products;     } } ```  In this code, we're injecting `IDistributedCache` into our `ProductService` class and using it to cache a list of products. We're using the `SetStringAsync` method to write the list of products to the cache and the `GetStringAsync` method to read the list of products from the cache.  ## Advanced Redis Caching Techniques  ### 1. Cache Serialization  For better performance, consider using a more efficient serialization method like Protocol Buffers or MessagePack instead of JSON:  ```csharp using MessagePack;  // Serialization var serializedProducts = MessagePackSerializer.Serialize(products);  // Deserialization var deserializedProducts = MessagePackSerializer.Deserialize>(cachedData); ```  ### 2. Distributed Locking  When multiple instances of your application are running, implement distributed locking to prevent race conditions:  ```csharp using StackExchange.Redis;  public async Task> GetProductsAsync() {     var cacheKey = \"ProductList\";     var lockKey = `${cacheKey}_lock`;     "},{"slug":"implementing-long-running-background-tasks-in-aspnet-core","title":"Implementing Long-Running Background Tasks in ASP.NET Core","description":"Learn how to implement long-running background tasks in ASP.NET Core using hosted services and background workers.","tags":["Aspnetcore","Background Tasks","Hosted Services","Dotnet"],"pubDate":"2024-10-26","content":" **Long-running background tasks are essential for performing operations that are not time-sensitive and can be executed asynchronously without blocking the main application thread.**  In this article, we will explore how to implement long-running background tasks in ASP.NET Core using hosted services and background workers. We will cover the following topics:  1. Why Use Background Tasks? 2. Implementing Background Tasks with IHostedService 3. Using BackgroundService for Simplicity 4. Handling Errors and Graceful Shutdown 5. Real-World Examples  ### 1. Why Use Background Tasks?  Background tasks are useful for performing operations such as:  - Sending emails - Processing data - Generating reports - Cleaning up resources - Running scheduled jobs  By offloading these tasks to background workers, you can improve the responsiveness and performance of your web application.  ### 2. Implementing Background Tasks with IHostedService  The `IHostedService` interface provides a way to implement background tasks in ASP.NET Core. It defines two methods: `StartAsync` and `StopAsync`.  ```csharp public class LongRunningService : IHostedService {     private readonly ILogger _logger;     private Timer _timer;      public LongRunningService(ILogger logger)     {         _logger = logger;     }      public Task StartAsync(CancellationToken cancellationToken)     {         _logger.LogInformation(\"Long Running Service is starting.\");         _timer = new Timer(DoWork, null, TimeSpan.Zero, TimeSpan.FromMinutes(1));         return Task.CompletedTask;     }      private void DoWork(object state)     {         _logger.LogInformation(\"Long Running Service is working.\");         // Perform long-running task here     }      public Task StopAsync(CancellationToken cancellationToken)     {         _logger.LogInformation(\"Long Running Service is stopping.\");         _timer?.Change(Timeout.Infinite, 0);         return Task.CompletedTask;     } } ```  ### 3. Using BackgroundService for Simplicity  The `BackgroundService` class is a base class for implementing long-running background tasks. It provides a simpler way to create background workers by overriding the `ExecuteAsync` method.  ```csharp public class BackgroundWorker : BackgroundService {     private readonly ILogger _logger;      public BackgroundWorker(ILogger logger)     {         _logger = logger;     }      protected override async Task ExecuteAsync(CancellationToken stoppingToken)     {         _logger.LogInformation(\"Background Worker is starting.\");          while (!stoppingToken.IsCancellationRequested)         {             _logger.LogInformation(\"Background Worker is working.\");             // Perform long-running task here             await Task.Delay(TimeSpan.FromMinutes(1), stoppingToken);         }          _logger.LogInformation(\"Background Worker is stopping.\");     } } ```  ### 4. Handling Errors and Graceful Shutdown  It's important to handle errors and ensure a graceful shutdown of background tasks. You can use try-catch blocks and the `stoppingToken` to handle cancellations.  ```csharp protected override async Task ExecuteAsync(CancellationToken stoppingToken) {     _logger.LogInformation(\"Background Worker is starting.\");      try     {         while (!stoppingToken.IsCancellationRequested)         {             _logger.LogInformation(\"Background Worker is working.\");             // Perform long-running task here             await Task.Delay(TimeSpan.FromMinutes(1), stoppingToken);         }     }     catch (Exception ex)     {         _logger.LogError(ex, \"An error occurred while executing the background task.\");     }     finally     {         _logger.LogInformation(\"Background Worker is stopping.\");     } } ```  ### 5. Real-World Scenarios  #### Sending Emails  You can use a background worker to send emails asynchronously.  ```csharp public class EmailSenderService : BackgroundService {     private readonly ILogger _logger;     private readonly IEmailSender _emailSender;      public EmailSenderService(ILogger logger, IEmailSender emailSender)     {         _logger = logger;         _emailSender = emailSender;     }      protected override async Task ExecuteAsync(CancellationToken stoppingToken)     {         _logger.LogInformation(\"Email Sender Service is starting.\");          while (!stoppingToken.IsCancellationRequested)         {             _logger.LogInformation(\"Sending emails.\");             await _emailSender.SendPendingEmailsAsync();             await Task.Delay(TimeSpan.FromMinutes(5), stoppingToken);         }          _logger.LogInformation(\"Email Sender Service is stopping.\");     } } ```  #### Data Processing  You can use a background worker to process data in the background.  ```csharp public class DataProcessingService : BackgroundService {     private readonly ILogger _logger;     private readonly IDataProcessor _dataProcessor;      public DataProcessingService(ILogger logger, IDataProcessor dataProcessor)     {         _logger = logger;  "},{"slug":"implementing-repository-pattern","title":"Implementing the Repository Pattern in .NET - A Game Changer for Clean Code","description":"Learn how to implement the Repository Pattern in .NET applications to achieve better separation of concerns, testability, and maintainability in your codebase.","tags":["Dotnet","Design-patterns","Clean-code","Architecture"],"pubDate":"2023-08-05","content":" The Repository Pattern is one of the most valuable design patterns for creating maintainable and testable .NET applications. It provides a clean separation between your data access logic and business logic, making your code more modular and easier to maintain.  ## What is the Repository Pattern?  The Repository Pattern is an abstraction that isolates the data layer from the rest of the application. It mediates between the domain and data mapping layers, acting like an in-memory collection of domain objects.  Key benefits include:  - **Separation of concerns**: Business logic is separated from data access logic - **Improved testability**: Easier to unit test business logic by mocking repositories - **Centralized data access logic**: Reduces duplication of query logic - **Flexibility**: Easier to switch between different data sources or ORMs  ## Basic Implementation  Let's start with a basic implementation of the Repository Pattern in a .NET application:  ### 1. Define the Entity  ```csharp public class Customer {     public int Id { get; set; }     public string Name { get; set; }     public string Email { get; set; }     public DateTime CreatedAt { get; set; } } ```  ### 2. Create the Repository Interface  ```csharp public interface IRepository where T : class {     IEnumerable GetAll();     T GetById(int id);     void Add(T entity);     void Update(T entity);     void Delete(T entity);     void SaveChanges(); } ```  ### 3. Implement the Generic Repository  ```csharp public class Repository : IRepository where T : class {     protected readonly DbContext _context;     protected readonly DbSet _dbSet;      public Repository(DbContext context)     {         _context = context;         _dbSet = context.Set();     }      public virtual IEnumerable GetAll()     {         return _dbSet.ToList();     }      public virtual T GetById(int id)     {         return _dbSet.Find(id);     }      public virtual void Add(T entity)     {         _dbSet.Add(entity);     }      public virtual void Update(T entity)     {         _dbSet.Attach(entity);         _context.Entry(entity).State = EntityState.Modified;     }      public virtual void Delete(T entity)     {         if (_context.Entry(entity).State == EntityState.Detached)         {             _dbSet.Attach(entity);         }         _dbSet.Remove(entity);     }      public void SaveChanges()     {         _context.SaveChanges();     } } ```  ### 4. Create Specific Repository (Optional)  For specific entities, you might want to extend the generic repository with custom methods:  ```csharp public interface ICustomerRepository : IRepository {     IEnumerable GetPremiumCustomers();     Customer GetByEmail(string email); }  public class CustomerRepository : Repository, ICustomerRepository {     public CustomerRepository(DbContext context) : base(context)     {     }      public IEnumerable GetPremiumCustomers()     {         return _dbSet.Where(c => c.IsPremium).ToList();     }      public Customer GetByEmail(string email)     {         return _dbSet.FirstOrDefault(c => c.Email == email);     } } ```  ## Using the Repository in Services  Now, let's see how to use the repository in a service class:  ```csharp public class CustomerService {     private readonly ICustomerRepository _customerRepository;      public CustomerService(ICustomerRepository customerRepository)     {         _customerRepository = customerRepository;     }      public void RegisterCustomer(string name, string email)     {         // Check if customer already exists         var existingCustomer = _customerRepository.GetByEmail(email);         if (existingCustomer != null)         {             throw new InvalidOperationException(\"Customer with this email already exists\");         }          // Create new customer         var customer = new Customer         {             Name = name,             Email = email,             CreatedAt = DateTime.UtcNow         };          // Add to repository         _customerRepository.Add(customer);         _customerRepository.SaveChanges();     }      public IEnumerable GetAllCustomers()     {         return _customerRepository.GetAll();     } } ```  ## Dependency Injection Setup  In ASP.NET Core, you can register your repositories in the `Startup.cs` file:  ```csharp public void ConfigureServices(IServiceCollection services) {     // Register DbContext     services.AddDbContext(options =>         options.UseSqlServer(Configuration.GetConnectionString(\"DefaultConnection\")));      // Register repositories     services.AddScoped(typeof(IRepository), typeof(Repository));     services.AddScoped();      // Register services     services.AddScoped();      // Other service registrations... } ```  ## Unit Testing with Mocked Repositories  One of the biggest advantages of the Repository Pattern is improved testability. Here's an example of how to unit test the `CustomerService` using a mocked repository:  ```csharp [Fact] public void RegisterCustomer_WithNewEmail_ShouldAddCustomer"},{"slug":"implementing-the-repository-pattern-in-dot-net-a-game-changer-for-clean-code","title":"Implementing the Repository Pattern in .NET: A Game-Changer for Clean Code","description":"Learn how to implement the repository pattern in .NET applications to simplify data access logic, improve code organization, and enhance maintainability.","tags":[".Net","C#","Design Patterns","Clean Code"],"pubDate":"2024-10-12","content":" In software development, particularly with .NET, the repository pattern can be a transformative tool for managing complex codebases. It simplifies data access logic, keeps the code clean, and ensures better organization in larger applications. Let's dive into what a repository is, why it's beneficial, and how to use it in your .NET applications.  ## What is a Repository?  Think of a repository as a middleman between your application and the database. Instead of embedding database access code all over your application, you centralize it in a repository. This not only keeps your code more organized but also abstracts the details of data access, allowing you to focus on the business logic.  In this article, we'll implement a repository pattern to manage **CRUD** (Create, Read, Update, Delete) operations for a game character in a .NET application using **Entity Framework**.  ## Setting Up the Project  We're using **ASP.NET Core Web API** with **Entity Framework Core** for this project. The project already includes:  - A **GameCharacter** entity representing a game character. - A **GameContext** class for the Entity Framework Core database context. - A **connection string** and **code-first migration** setup.  Now, let's organize the data access code with a repository pattern.  ## Step 1: Define the Repository Interface  We'll start by creating an interface that defines the repository's capabilities. This interface will act as a contract for any class implementing the repository.  ```csharp public interface IGameCharacterRepository {     List GetAllCharacters();     GameCharacter GetCharacterById(int id);     void AddCharacter(GameCharacter character);     void UpdateCharacter(GameCharacter character);     void DeleteCharacter(int id); } ```  ### Step 2: Implement the Repository  Next, we’ll create the `GameCharacterRepository` class that implements the `IGameCharacterRepository` interface. This class will handle data access via Entity Framework.  ```csharp  public class GameCharacterRepository : IGameCharacterRepository {     private readonly GameContext _context;      public GameCharacterRepository(GameContext context)     {         _context = context;     }      public List GetAllCharacters()     {         return _context.GameCharacters.ToList();     }      public GameCharacter GetCharacterById(int id)     {         return _context.GameCharacters.Find(id);     }      public void AddCharacter(GameCharacter character)     {         _context.GameCharacters.Add(character);         _context.SaveChanges();     }      public void UpdateCharacter(GameCharacter character)     {         var characterToUpdate = _context.GameCharacters             .FirstOrDefault(c => c.Id == character.Id);         if (characterToUpdate == null) return;          characterToUpdate.Name = character.Name;         characterToUpdate.Health = character.Health;         characterToUpdate.Level = character.Level;         characterToUpdate.Weapon = character.Weapon;          _context.SaveChanges();     }      public void DeleteCharacter(int id)     {         var character = _context.GameCharacters.Find(id);         if (character == null) return;          _context.GameCharacters.Remove(character);         _context.SaveChanges();     } }  ```  ### Step 3: Use Dependency Injection  In **ASP.NET Core**, we can inject services like the `GameCharacterRepository` into the controllers through **Dependency Injection**. First, register the repository in `Program.cs`.  ```csharp  builder.Services.AddScoped();  ```  This tells the application to inject the `GameCharacterRepository` whenever `IGameCharacterRepository` is required.  ### Step 4: Create the Controller  The final step is to use this repository in our controller. Let’s create an `APIController` to handle requests related to game characters.  ```csharp  [ApiController] [Route(\"api/[controller]\")] public class GameCharactersController : ControllerBase {     private readonly IGameCharacterRepository _repository;      public GameCharactersController(IGameCharacterRepository repository)     {         _repository = repository;     }      [HttpGet]     public IActionResult GetAllCharacters()     {         var characters = _repository.GetAllCharacters();         return Ok(characters);     }      [HttpGet(\"{id}\")]     public IActionResult GetCharacterById(int id)     {         var character = _repository.GetCharacterById(id);         if (character == null) return NotFound();         return Ok(character);     }      [HttpPost]     public IActionResult AddCharacter(GameCharacter character)     {         _repository.AddCharacter(character);         return CreatedAtAction(nameof(GetCharacterById), new { id = character.Id }, character);     }      [HttpPut(\"{id}\")]     public IActionResult UpdateCharacter(int id, GameCharacter character)     {         if (id != character.Id) return BadRequest();         _repository.UpdateCharacter(character);         return NoContent();     }      [HttpDelete(\"{id}\")]     publi"},{"slug":"integrating_elastic_search_with_dotnet_web_api","title":"Integrating ElasticSearch with .NET Web API","description":"This guide will walk you through setting up ElasticSearch locally using Docker Compose, connecting to ElasticSearch from your .NET Web API, and creating simple CRUD operations.","tags":["Elasticsearch","Search","Analytics","Crud"],"pubDate":"2024-08-25","content":" ElasticSearch offers several benefits, including high-performance search, real-time search, full-text search, faceting, geolocation search, analytics capabilities, ease of use, scalability, reliability, and open-source nature. These features make it a popular choice for search and analytics applications, as it can handle large datasets, provide fast and accurate results, and be easily integrated into different systems.  ## Setting up ElasticSearch Locally using Docker Compose  Let's begin by creating a new .NET project and adding a new file named `docker-compose.yml` to the root of the project. This file will define the services used in the Docker Compose environment.  ```yaml version: '3.8' services:   elasticsearch:     container_name: els     image: elasticsearch:8.15.0     ports:       - \"9200:9200\"     volumes:       - elasticsearch-data:/usr/share/elasticsearch/data     environment:       - discovery.type=single-node       - xpack.security.enabled=false     networks:       - elk   kibana:     container_name: kibana     image: kibana:8.15.0     ports:       - \"5601:5601\"     depends_on:       - elasticsearch     environment:       - ELASTICSEARCH_URL=http://elasticsearch:9200     networks:       - elk networks:   elk:     driver: bridge volumes:   elasticsearch-data: ```  This `docker-compose.yml` file defines two services: Elasticsearch and Kibana.  The Elasticsearch service is configured to run a single node of Elasticsearch version 8.15.0. It maps port 9200 of the container to port 9200 of the host machine. The `volumes` section defines a volume named `elasticsearch-data` that will store the data of the Elasticsearch service. The `environment` section disables Elasticsearch's security features.  The Kibana service is configured to run Kibana version 8.15.0. It maps port 5601 of the container to port 5601 of the host machine. Kibana is a web interface for Elasticsearch that allows you to visualize and interact with your data.  To start the Elasticsearch and Kibana services, open a terminal window and run the following command:  ```bash docker-compose up ```  This command will start the Elasticsearch and Kibana services in the background. Once the services are running, you can access Kibana by navigating to `http://localhost:5601` in your web browser.  ## Integrating ElasticSearch with a .NET Web API  Create a new .NET Web API project and install the following NuGet package:  ```bash dotnet add package Elastic.Clients.Elasticsearch ```  This package provides the necessary classes to interact with Elasticsearch from .NET.  Next, create a new folder named `Models` and add a new class named `User`:  ```csharp public class User {     [JsonProperty(\"Id\")]     public int Id { get; set; }      [JsonProperty(\"FirstName\")]     public string FirstName { get; set; }      [JsonProperty(\"LastName\")]     public string LastName { get; set; } } ```  Then create a new folder named `Services` and add a new interface named `IElasticService`:  ```csharp public interface IElasticService {     Task CreateIndexIfNotExistsAsync(string indexName);     Task AddOrUpdate(User user);     Task AddOrUpdateBulk(IEnumerable users, string indexName);     Task Get(string key);     Task> GetAll();     Task Remove(string key);     Task RemoveAll(); } ```  Add a new class named `ElasticService` to the `Services` folder to implement the `IElasticService` interface:  ```csharp public class ElasticService : IElasticService {     private readonly ElasticsearchClient _client;     private readonly ElasticSettings _elasticSettings;      public ElasticService(IOptions options)     {         _elasticSettings = options.Value;         var settings = new ElasticsearchClientSettings(new Uri(_elasticSettings.Url))             .DefaultIndex(_elasticSettings.DefaultIndex);         _client = new ElasticsearchClient(settings);     }      public async Task CreateIndexIfNotExistsAsync(string indexName)     {         if (!_client.Indices.Exists(indexName).Exists)         {             await _client.Indices.CreateAsync(indexName);         }     }      public async Task AddOrUpdate(User user)     {         var response = await _client.IndexAsync(user, idx => idx             .Index(_elasticSettings.DefaultIndex)             .Id(user.Id)             .Refresh(Refresh.WaitFor));         return response.IsValidResponse;     }      public async Task AddOrUpdateBulk(IEnumerable users, string indexName)     {         var response = await _client.BulkAsync(b => b             .Index(_elasticSettings.DefaultIndex)             .UpdateMany(users, (ud, u) => ud.Doc(u).DocAsUpsert(true)));         return response.IsValidResponse;     }      public async Task Get(string key)     {         var response = await _client.GetAsync(key,             g => g.Index(_elasticSettings.DefaultIndex));         return response.Source;     }      public async Task> GetAll()     {         var response = await _client.SearchAsync(s => s             .Index(_elasticSettings.DefaultIndex));       "},{"slug":"llama-4-maverick-the-evolution-in-open-weight-models","title":"Llama 4 Maverick: The Next Evolution in Open-Weight AI Models","description":"Explore the capabilities, advantages, and limitations of Llama 4 Maverick, Meta's latest multimodal AI model, and its comparison with other state-of-the-art models.","tags":["Llama4","Multimodal Ai","Meta Ai","Artificial Intelligence"],"pubDate":"2025-04-07","content":" ![Llama 4 Maverick model architecture showing the mixture-of-experts design](https://www.geeky-gadgets.com/wp-content/uploads/2025/04/meta-llama-4-maverick-scout-variants.webp)  Meta has recently unveiled its latest generation of large language models with the release of Llama 4, introducing two powerful variants: Llama 4 Maverick and Llama 4 Scout. These models represent a significant leap forward in open-weight AI technology, bringing impressive capabilities that challenge even the most advanced proprietary models on the market. This article explores Llama 4 Maverick's architecture, capabilities, advantages over competing models, and its limitations.  ## What is Llama 4 Maverick?  Llama 4 Maverick is a state-of-the-art multimodal AI model developed by Meta, released on April 5, 2025. It's part of Meta's new Llama 4 collection, which also includes Scout (a smaller model) and Behemoth (a larger unreleased model used for distillation).  Llama 4 Maverick is Meta's first model to use a mixture-of-experts (MoE) architecture, with 17 billion active parameters and approximately 400 billion total parameters. The model features 128 experts, with only a small subset of these experts activated for any given input token, making it computationally efficient while maintaining high performance.  As Meta describes it: \"Llama 4 Maverick, a 17 billion active parameter model with 128 experts, is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks, while achieving comparable results to the new DeepSeek v3 on reasoning and coding—at less than half the active parameters.\" [Meta AI](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)  ## Groundbreaking Technical Architecture  ### Mixture-of-Experts Design  One of the most significant advancements in Llama 4 Maverick is its Mixture-of-Experts (MoE) architecture. Unlike traditional \"dense\" AI models where every input flows through every parameter, Maverick employs a selective approach:  - **128 Routed Experts + 1 Shared Expert**: The model features 128 specialized \"expert\" neural networks plus one shared expert. - **Selective Activation**: For any given token (word or image element), only the shared expert and one of the 128 specialized experts are activated, meaning only a fraction of the model's total parameters are used during processing. - **Router Mechanism**: A specialized neural network called a router determines which expert should process each token based on the token's content.  This architecture allows Maverick to have the knowledge capacity of a much larger model (400B parameters) while maintaining the inference speed of a much smaller model (17B active parameters). As noted in a technical analysis: \"This improves inference efficiency by lowering model serving costs and latency—Llama 4 Maverick can be run on a single NVIDIA H100 DGX host for easy deployment, or with distributed inference for maximum efficiency.\" [NVIDIA Developer](https://developer.nvidia.com/blog/nvidia-accelerates-inference-on-meta-llama-4-scout-and-maverick/)  ### Native Multimodality with Early Fusion  Llama 4 Maverick is built from the ground up to understand both text and images natively, rather than having visual capabilities \"bolted on\" as an afterthought:  - **Early Fusion Architecture**: Text and visual information are integrated at a fundamental level, allowing the model to process both simultaneously. - **Enhanced Vision Encoder**: Based on MetaCLIP but specially trained with a frozen Llama model to better adapt visual information into the language model. - **Cross-Modal Understanding**: The model can draw direct connections between elements in text and corresponding visual elements in images.  This enables more sophisticated image understanding than previous models, allowing Maverick to process up to eight images at once with good results, and it was pre-trained on up to 48 images.  ### Advanced Post-Training Pipeline  Llama 4 models use a novel three-stage approach to fine-tuning:  1. **Lightweight Supervised Fine-Tuning (SFT)**: The team removed over 50% of \"easy\" training examples to focus on more challenging tasks. 2. **Online Reinforcement Learning (RL)**: A continuous learning process focusing on harder prompts to improve reasoning and coding. 3. **Lightweight Direct Preference Optimization (DPO)**: A final refinement stage to handle edge cases and response quality.  This approach enables better preservation of complex reasoning capabilities than traditional methods, allowing the model to excel at both technical tasks and conversational abilities.  ## Performance Advantages Over Other Models  ### Benchmark Results  According to Meta's published benchmarks, Llama 4 Maverick demonstrates impressive capabilities across multiple domains:  - **Reasoning & Knowledge**: 80.5% on MMLU Pro and 69.8% on GPQA Diamond - **Coding**: 43.4% pass@1 on LiveCodeBench (Oct 2024-Feb 2025) - **Image Understanding**: 90.0% "},{"slug":"master-vibe-coding-ai-powered-tips-software-development","title":"Master Vibe Coding: 21 AI-Powered Tips for Effortless Software Development","description":"Discover 21 expert tips to supercharge your AI-assisted development workflow, from foundational strategies to advanced techniques for seamless software creation.","tags":["ai","programming","productivity","development","vibecoding","llm"],"pubDate":"2025-05-19","content":" Hey there, fellow code wranglers! If you've been keeping up with the dev scene lately, you've probably heard the buzz around \"vibe coding\" – that magical approach where AI takes the wheel (or at least rides shotgun) in your software development journey.   The promise is enticing: build faster, innovate more, and get your dream SaaS off the ground without the usual headaches. But as many of us have discovered, this exciting new paradigm can quickly turn into a source of immense frustration if you don't know what you're doing.  The core issue? As Sean, the host of the video this article is based on, puts it: \"You don't know what you don't know.\" Without a solid understanding of software engineering principles and how to effectively guide AI, you're likely to hit roadblocks that leave you questioning whether this whole vibe coding thing is worth the hype.  But don't worry – I've got your back. This article distills Sean's 21 pro tips, gleaned from his experience running businesses that leverage AI, to help you navigate the world of vibe coding more effectively. Whether you're just starting out or looking to refine your AI-assisted workflow, these insights will help you avoid common pitfalls and maximize your productivity.  Let's dive in!  ![AI-Powered Coding Tips](https://ik.imagekit.io/quadrate/assets/img/dotnetevangelist/ai-powered-coding-tipcs.png?updatedAt=1747591094656)  ## Foundational Strategies for Better AI Coding  ### Tip 1: Leverage Widely Documented Tech Stacks  Large Language Models (LLMs) like Claude and GPT are trained on vast amounts of data, including code examples. This means they perform best when working with technologies they've seen frequently in their training data.  When you use popular, well-documented technologies (React, FastAPI, Supabase, Firebase, Stripe, AWS Amplify, etc.), the AI has more relevant training data to draw from. This leads to more accurate code generation and fewer \"hallucinations\" (incorrect or nonsensical outputs).  The benefit? Reduced errors, faster development, and more reliable AI-generated code. It's like giving your AI assistant a familiar toolbox rather than asking it to work with obscure instruments it's barely seen before.  ### Tip 2: Plan Extensively Ahead of Time  While AI can write code, it's not (yet) a master architect for complex systems, especially when trying to design and implement simultaneously.  LLMs have context window limitations. Overwhelming them with too much information at once degrades performance. That's why Sean suggests dedicating a significant portion of your development time (60-70%) to planning before writing any code.  This planning should include: - Defining the tech stack - Mapping out the application architecture - Identifying where different pieces of code will reside - Acknowledging your own knowledge gaps - Detailing main requirements for each feature - Writing complete user stories, outlining interactions from A to Z - Considering state management, API endpoints, payloads, database schemas, and component logic  The result? Clearer direction for the AI, more focused prompts, and ultimately, a more robust and well-structured application.  ### Tip 3: Use Context Wisely  AI development tools often allow you to specify context for your prompts using \"@\" symbols or similar features.  Since LLMs have maximum context limits, feeding the entire codebase for every request is inefficient and can lead to the AI missing crucial details. Be selective. Provide only the necessary files, folders, or documentation relevant to the current task.  This ensures the AI focuses its processing power on the most relevant information, leading to better, more targeted results. For example, Cursor IDE's \"Add context\" feature allows selecting specific files or folders, giving you fine-grained control over what information the AI considers.  ### Tip 4: Commit Between Conversations  The practice of frequent, small commits is a cornerstone of traditional software development and is even more critical in vibe coding.  Creating snapshots of your codebase means if the AI generates code that breaks your application, you can easily roll back to a working version. It also facilitates working on different features in parallel.  Consider the end of a \"conversation\" with the AI (after a specific feature or task is completed) as a natural point to commit. You can even ask the AI to help generate a commit message that accurately describes the changes made.  ### Tip 5: Commit Solutions to (System) Memory  Don't solve the same problem repeatedly. Many AI IDEs have a \"memory\" system where you can store common solutions, code snippets, or styling preferences.  When you and the AI devise a good solution to a recurring problem (a specific debugging technique, a styling pattern), commit it to the IDE's memory. Tools like Windsurf (with \"Memories\") or those supporting MCP (Model Context Protocol) servers offer this functionality.  This approach saves significant time and ef"},{"slug":"mastering-common-table-expressions-ctes-in-postgresql-recursive-queries-performance-tips","title":"Mastering Common Table Expressions (CTEs) in PostgreSQL: Recursive Queries and Performance Tips","description":"A comprehensive guide to Common Table Expressions (CTEs) in PostgreSQL, including recursive queries, practical use cases, and performance optimization tips for writing efficient, maintainable SQL.","tags":["Postgresql","Sql","Cte","Database","Performance","Recursion"],"pubDate":"2025-05-11","content":" Common Table Expressions (CTEs) are one of the most powerful tools in PostgreSQL for writing clean, maintainable, and advanced SQL queries. Whether you’re working with simple data transformations or tackling complex hierarchical data, CTEs can help you break down problems, improve readability, and unlock advanced capabilities like recursion. In this in-depth article, we'll explore the syntax, practical use cases, recursive queries, performance implications, and best practices for CTEs in PostgreSQL.  ## What is a CTE?  A **Common Table Expression** (CTE) is a temporary result set that you can reference within a single SQL statement. CTEs are defined using the `WITH` clause and provide a way to organize your queries into logical building blocks, making them easier to read, debug, and maintain.  ### Basic CTE Syntax  ```sql WITH cte_name AS (   SELECT ... ) SELECT * FROM cte_name; ```  You can define multiple CTEs by separating them with commas:  ```sql WITH cte1 AS (...),      cte2 AS (...) SELECT ... FROM cte1 JOIN cte2 ON ...; ```  ## Why Use CTEs?  CTEs offer several key advantages for SQL developers:  - **Readability:** Break down complex logic into manageable steps. - **Reusability:** Reference the same CTE multiple times in a query. - **Recursion:** Solve hierarchical or graph problems elegantly. - **Maintainability:** Isolate subqueries for easier debugging and refactoring. - **Modularity:** Compose queries in a modular, stepwise fashion.  ### When to Use CTEs  Use CTEs when: - You need to reuse a subquery multiple times. - You want to improve the clarity of deeply nested queries. - You are working with hierarchical data (e.g., organization charts, category trees). - You need to perform recursive operations (e.g., traversing parent-child relationships).  ## Practical Example: Simplifying a Complex Query  Suppose you want to find employees in each department whose salaries are above their department’s average. Without CTEs, this can become a deeply nested subquery. With CTEs, the logic is much clearer:  ```sql WITH dept_avg AS (   SELECT department_id, AVG(salary) AS avg_salary   FROM employees   GROUP BY department_id ) SELECT e.employee_id, e.name, e.salary, e.department_id FROM employees e JOIN dept_avg d ON e.department_id = d.department_id WHERE e.salary > d.avg_salary; ```  This approach makes your SQL more readable and easier to maintain, especially as business logic grows.  ## Recursive CTEs: Unlocking Hierarchical and Graph Data  One of the most compelling features of CTEs in PostgreSQL is recursion. Recursive CTEs allow you to traverse hierarchical data structures, such as organization charts, folder trees, or bill-of-materials relationships, directly in SQL.  ### Recursive CTE Syntax  A recursive CTE consists of two parts: - **Anchor member:** The base case of the recursion (e.g., top-level managers). - **Recursive member:** A query that references the CTE itself, building upon the anchor set.  ```sql WITH RECURSIVE cte_name AS (   -- Anchor member   SELECT ...   UNION ALL   -- Recursive member   SELECT ... FROM cte_name JOIN ... ) SELECT * FROM cte_name; ```  ### Example: Organizational Hierarchy  Suppose you have an `employees` table with `employee_id`, `manager_id`, and `name` columns. To build an org chart:  ```sql WITH RECURSIVE org_chart AS (   SELECT employee_id, manager_id, name, 1 AS level   FROM employees   WHERE manager_id IS NULL   UNION ALL   SELECT e.employee_id, e.manager_id, e.name, oc.level + 1   FROM employees e   INNER JOIN org_chart oc ON e.manager_id = oc.employee_id ) SELECT * FROM org_chart ORDER BY level, employee_id; ```  This query starts with top-level managers and recursively finds all subordinates, assigning each a `level` in the hierarchy.  #### Real-World Recursive Use Cases - **Category trees:** E-commerce product categories with parent-child relationships. - **File systems:** Navigating folder structures. - **Graph traversal:** Finding all paths or cycles in a network. - **Bill of materials:** Expanding product components in manufacturing.  ## Performance Tips for CTEs in PostgreSQL  While CTEs are powerful, they can also introduce performance pitfalls if not used carefully. Here are some tips to keep your queries efficient:  ### 1. Materialization vs. Inlining - **Materialization (Pre-PostgreSQL 12):** CTEs are always executed and stored before the main query, which can prevent the optimizer from reordering or merging them for efficiency. - **Inlining (PostgreSQL 12+):** By default, PostgreSQL may inline CTEs, treating them like subqueries for better performance. You can control this with `MATERIALIZED` or `NOT MATERIALIZED` hints:  ```sql WITH cte_name AS MATERIALIZED (   SELECT ... ) SELECT * FROM cte_name; ```  ### 2. Use Indexes Ensure that columns used in CTE joins and filters are indexed. Poor indexing can lead to slow query execution, especially with large datasets.  ### 3. Limit Recursive Depth When working with recursive CTEs, always include a dept"},{"slug":"mastering-kubernetes-google-cloud","title":"Mastering Kubernetes on Google Cloud: A Comprehensive Guide","description":"Dive deep into deploying, managing, and scaling containerized applications using Kubernetes on Google Cloud Platform (GCP). This comprehensive guide covers GKE architecture, detailed setup, operational best practices, advanced deployment strategies, and real-world examples visualized with diagrams.","tags":["Kubernetes","Google Cloud","GKE","DevOps","Containers","Microservices","CI/CD"],"pubDate":"2025-05-12","content":" Kubernetes has revolutionized how we manage containerized applications, establishing itself as the de facto standard for orchestration. Google Cloud Platform (GCP) offers a powerful, managed Kubernetes service called Google Kubernetes Engine (GKE), which simplifies cluster operations and integrates seamlessly with the GCP ecosystem. This guide will walk you through leveraging Kubernetes on Google Cloud, from understanding core concepts and initial setup to advanced deployment strategies and operational best practices, ensuring your applications are scalable, resilient, and efficiently managed.  ### Understanding Core Concepts: Kubernetes and GKE  Before diving into GKE, let's clarify what Kubernetes is and how GKE builds upon it.  #### Kubernetes: The Container Orchestrator  Kubernetes (often abbreviated as K8s) is an open-source platform designed to automate the deployment, scaling, and management of containerized applications. It groups containers that make up an application into logical units for easy management and discovery.  **Key Kubernetes Components:**  *   **Control Plane:** The brain of the cluster. It manages worker nodes and Pods in the cluster. Components include:     *   `kube-apiserver`: Exposes the Kubernetes API.     *   `etcd`: Consistent and highly-available key-value store used as Kubernetes' backing store for all cluster data.     *   `kube-scheduler`: Watches for newly created Pods with no assigned node and selects a node for them to run on.     *   `kube-controller-manager`: Runs controller processes. *   **Nodes (Worker Machines):** VMs or physical machines where your containerized applications run. Each node contains:     *   `kubelet`: An agent that runs on each node. It makes sure that containers are running in a Pod.     *   `kube-proxy`: A network proxy that runs on each node, implementing part of the Kubernetes Service concept.     *   Container Runtime: Software responsible for running containers (e.g., Docker, containerd). *   **Pods:** The smallest deployable units of computing that you can create and manage in Kubernetes. A Pod represents a single instance of a running process in your cluster and can contain one or more containers. *   **Services:** An abstract way to expose an application running on a set of Pods as a network service. *   **Deployments:** Provides declarative updates for Pods and ReplicaSets (which ensure a specified number of Pod replicas are running).  ```mermaid graph TD     subgraph ControlPlane[\"Control Plane\"]         APIServer[\"kube-apiserver\"]         ETCD[\"etcd (Cluster State Store)\"]         Scheduler[\"kube-scheduler\"]         ControllerManager[\"kube-controller-manager\"]     end      subgraph Node1Worker[\"Node 1 (Worker)\"]         Kubelet1[\"kubelet\"]         KubeProxy1[\"kube-proxy (Manages Network Rules on Node)\"]         ContainerRuntime1[\"Container Runtime (e.g., Docker, containerd)\"]         Pod1[\"Pod (App 1 Container)\"]         Pod2[\"Pod (App 2 Container)\"]     end      subgraph Node2Worker[\"Node 2 (Worker)\"]         Kubelet2[\"kubelet\"]         KubeProxy2[\"kube-proxy (Manages Network Rules on Node)\"]         ContainerRuntime2[\"Container Runtime (e.g., Docker, containerd)\"]         Pod3[\"Pod (App 3 Container)\"]     end      User[\"User/kubectl\"] -- \"Sends API Requests\" --> APIServer     APIServer -- \"Reads/Writes Cluster State\" --- ETCD     ControllerManager -- \"Watches Resources &Acts via API Server\" --> APIServer     Scheduler -- \"Watches for Unscheduled Pods &Assigns Node (via API Server)\" --> APIServer      APIServer -- \"Desired State (Pod Specs, etc.)\" --> Kubelet1     APIServer -- \"Desired State (Pod Specs, etc.)\" --> Kubelet2          APIServer -- \"Service/Endpoint Info\" --> KubeProxy1     APIServer -- \"Service/Endpoint Info\" --> KubeProxy2      Kubelet1 -- \"Manages Pod Lifecycle\" --> Pod1     Kubelet1 -- \"Manages Pod Lifecycle\" --> Pod2     Kubelet2 -- \"Manages Pod Lifecycle\" --> Pod3          Kubelet1 -- \"Instructs\" --> ContainerRuntime1     Kubelet2 -- \"Instructs\" --> ContainerRuntime2      style ControlPlane fill:#f9f,stroke:#333,stroke-width:2px     style Node1Worker fill:#ccf,stroke:#333,stroke-width:2px     style Node2Worker fill:#ccf,stroke:#333,stroke-width:2px ```  #### Google Kubernetes Engine (GKE): Managed Kubernetes on GCP  GKE is Google Cloud’s managed Kubernetes service. It reduces operational overhead by handling much of the cluster management lifecycle, including provisioning, scaling, upgrades, and maintenance of the control plane and, optionally, nodes.  **Key Benefits of Using GKE:**  *   **Managed Infrastructure**: Google manages the Kubernetes control plane (master nodes), ensuring high availability and reliability. For GKE Autopilot clusters, Google also manages the worker nodes. *   **Seamless Integration**: Native integration with Google Cloud services like Artifact Registry (for container images), Cloud Monitoring, Cloud Logging, Identity and Access Management (IAM), Virtual Private Cloud (VPC), Cloud Storage"},{"slug":"mastering-sql-the-power-of-sum-with-case-when","title":"Mastering SQL: The Power of SUM() with CASE WHEN","description":"Unlock the potential of conditional aggregation in SQL using SUM() with CASE WHEN. Learn how to extract meaningful insights from your data with ease.","tags":["Sql","Data Analysis","Programming"],"pubDate":"2024-10-11","content":" **Discover how combining SUM() with CASE WHEN can revolutionize your data analysis in SQL.**  ## Introduction  In the ever-evolving landscape of data analysis, SQL (Structured Query Language) continues to stand as a cornerstone for extracting insights from structured datasets. As the volume and complexity of data grow exponentially, so does the need for more sophisticated analytical techniques. Enter the powerful combination of the `SUM()` function with the `CASE WHEN` clause – a game-changing approach that opens up a world of possibilities for your analytical queries.  This article delves deep into this dynamic duo, exploring how it enables flexible, condition-based aggregation of data. Whether you're a seasoned data analyst or just starting your journey in SQL, understanding this technique will significantly enhance your ability to derive meaningful insights from your data.  ## The Power of Conditional Aggregation  Imagine you're tasked with analyzing a massive dataset containing millions of records. Your goal is to count occurrences or calculate percentages based on specific criteria. Traditional aggregate functions like `COUNT()` or `AVG()` might seem like the go-to solution, but they often fall short when dealing with complex conditional logic. This is where the combination of `SUM()` with `CASE WHEN` truly shines.  ### How It Works  The magic of this technique lies in its simplicity and flexibility. Let's break down the process into three key steps:  1. **Set Your Conditions**: The `CASE WHEN` clause allows you to specify precise criteria for which rows should be included in your aggregation. This could be based on any column in your dataset – from categorical values to numerical ranges or even date periods.  2. **Count the Matches**: The `SUM()` function works in tandem with `CASE WHEN` to add up the rows that meet your specified conditions. Typically, this involves adding 1 for matching rows and 0 for non-matching rows, effectively creating a count of matching records.  3. **Calculate Percentages**: By dividing your conditional sum by the total count of rows, you can easily derive percentages or proportions. This step transforms raw counts into meaningful metrics that provide valuable insights into your data.  ## Real-World Example: Color Analysis  To illustrate the power of this technique, let's dive into a practical example. Imagine we have a table called `product_inventory` that contains information about various items, including their colors:  ```sql CREATE TABLE product_inventory (     id INT PRIMARY KEY,     product_name VARCHAR(100),     color VARCHAR(50),     date_added DATE );  INSERT INTO product_inventory (id, product_name, color, date_added) VALUES     (1, 'T-shirt', 'red', '2023-01-01'),     (2, 'Jeans', 'blue', '2023-01-02'),     (3, 'Sweater', 'red', '2023-01-03'),     (4, 'Dress', 'green', '2023-01-04'),     (5, 'Jacket', 'red', '2023-01-05'); ```  Now, let's say we want to calculate the percentage of items in our inventory that are red. Here's how we can do that using `SUM()` with `CASE WHEN`:  ```sql SELECT     SUM(CASE WHEN color = 'red' THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS percentage_red FROM     product_inventory; ```  This query will return:  ``` percentage_red -------------- 60.00 ```  This result tells us that 60% of the items in our inventory are red. Pretty cool, right? But we can take this analysis even further!  ### Time-Based Analysis  What if we want to see how the color distribution of our inventory changes over time? No problem! We can modify our query to group the results by year:  ```sql SELECT     EXTRACT(YEAR FROM date_added) AS year,     SUM(CASE WHEN color = 'red' THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS percentage_red FROM     product_inventory GROUP BY     EXTRACT(YEAR FROM date_added) ORDER BY     year; ```  This query would show us the percentage of red items added to our inventory each year. If we had data spanning multiple years, we could observe trends and patterns in our inventory composition over time.  ## Why This Matters  The `SUM()` with `CASE WHEN` technique isn't just a neat trick – it's a powerful tool with real-world applications across various industries:  - **Business Analytics**: - Track product return rates across different brands or regions - Analyze the percentage of high-value customers in different market segments - Calculate the proportion of orders that meet specific criteria (e.g., rush orders, bulk orders)  - **Healthcare**: - Analyze the prevalence of certain symptoms among different patient groups - Calculate the effectiveness of treatments across various demographics - Track the percentage of patients adhering to prescribed medication regimens  - **Marketing**: - Measure campaign success rates across different channels - Analyze customer engagement levels for various product categories - Calculate conversion rates for different marketing strategies  - **Finance**: - Analyze the percentage of transactions flagged for potential "},{"slug":"new-era-of-onedrive-with-redesigned-interface","title":"New Era of OneDrive with Redesigned Interface","description":"Explore OneDrive's latest AI-powered features, including Copilot Agents, enhanced productivity tools, and a revamped mobile experience for both work and personal use.","tags":["Onedrive","Microsoft","Ai","Productivity","File Management","Collaboration"],"pubDate":"2024-08-28","content":" ## OneDrive Transforms File Management and Photo Storage with AI Power  Microsoft OneDrive, the trusted file storage and sharing platform used by Fortune 500 companies, small businesses, and individuals alike, has unveiled a suite of AI-powered features designed to revolutionize how we work, collaborate, and preserve memories.  This article breaks down the exciting new capabilities announced at the recent OneDrive event.  **Introducing Copilot Agents: Your Personalized AI Assistants**   Imagine having a dedicated AI assistant tailored to your specific project needs.  That's precisely what Copilot Agents offer within OneDrive.  These custom-built AI agents can be trained with relevant files, documents, and data, transforming them into powerful, shareable resources.  * **Creating Your AI Agent:** Building a Copilot Agent is simple.  Gather the necessary files, give your agent a unique name and icon for easy recognition, and it's ready to go.  * **Automating Tasks:** Copilot Agents excel at handling repetitive tasks.  For instance, during conference planning, your agent can automatically respond to common queries like \"What's the agenda?\" or \"Show me the budget summary,\" ensuring everyone has access to crucial information around the clock.  * **Seamless Sharing and Scalability:** Copilot Agents are designed for collaboration. Share them with your team across various platforms like Microsoft Teams, OneDrive, and SharePoint.  Onboarding new team members becomes effortless—simply share the agent, and they'll have instant access to all the project knowledge.  * **Revolutionizing Collaboration:** Copilot Agents streamline knowledge transfer and enhance teamwork. Treat them like any other file – copy, share, and organize them as needed.  This simplifies project management and fosters efficient communication.  **Boosting Productivity with a Smarter OneDrive at Work** ![Smarter OneDrive](https://ik.imagekit.io/quadrate/assets/img/dotnetevangelist/new-era-of-onedrive-with-redesigned-interface.jpg?updatedAt=1746813301185)  OneDrive is deeply integrated with the Microsoft 365 ecosystem, working seamlessly with Office, Teams, SharePoint, and Windows.  The latest updates enhance this integration, providing a faster, smarter, and more collaborative work experience.  * **Lightning-Fast Performance:** OneDrive's web experience has been supercharged.  Enjoy rapid file viewing, editing, and organizing, even offline.  Large libraries load quickly, and filtering documents is smoother than ever.  * **Enhanced Search:**  Find files and folders effortlessly with the improved search experience.  New filter controls and detailed results pinpoint exactly what you need, regardless of where it's stored—your OneDrive, a shared library, or a teammate's folder.  This update is progressively rolling out and will be widely available by the end of 2024.  * **Personalized Organization with Colored Folders:** Add a touch of personality and improve organization with colored folders in Windows File Explorer.  This visually appealing feature, available now, simplifies file management.  * **Modernized Document Libraries:**  Expect significant improvements to document libraries mid-next year.  OneDrive's speed and flexibility will be integrated into these collaborative spaces, boosting performance, navigation, and customization of views, especially for large datasets.  * **Streamlined Sharing:** Real-time visibility of who's working on a document, coupled with the ability to set expiration dates for sharing links, strengthens control and security.  **Harnessing the Power of Copilot in OneDrive** ![Copilot in OneDrive](https://ik.imagekit.io/quadrate/assets/img/dotnetevangelist/new-era-of-onedrive-with-redesigned-interface.jpg?updatedAt=1746813301185)  Copilot in OneDrive, now available for commercial users, leverages AI to unlock unprecedented productivity.  * **Instant Summarization:**  Summarize single or multiple files within OneDrive's web app without opening them individually.  Quickly extract key information or generate FAQs for sharing, streamlining your workflow.  * **Effortless File Comparison:** Compare up to five files of various formats (Word, PowerPoint, PDF) simultaneously to quickly identify differences and make informed decisions.  * **File-Specific Insights:** Ask Copilot questions about specific documents without opening them.  Gain valuable insights in seconds, preparing for meetings or quickly understanding document content.  * **Coming in 2025:**  Look forward to features like meeting recaps directly within OneDrive, providing actionable insights from meeting highlights, and automatic conversion of Word documents into PowerPoint presentations.  **Enhanced Security and Management for IT Professionals** ![Copilot Agents](https://ik.imagekit.io/quadrate/assets/img/dotnetevangelist/new-era-of-onedrive-with-redesigned-interface.jpg?updatedAt=1746813301185)  OneDrive prioritizes security and manageability, particularly for IT administ"},{"slug":"performance-optimization-techniques-in-python","title":"Performance Optimization Techniques in Python","description":"A comprehensive guide to performance optimization techniques in Python for beginners and advanced users.","tags":["Python","Performance","Optimization","Programming"],"pubDate":"2024-10-15","content":" **Optimizing performance in Python is crucial for developing efficient and scalable applications.**  ## Introduction  Python is a versatile and widely-used programming language known for its simplicity and readability. However, Python's dynamic nature can sometimes lead to performance bottlenecks. This article explores various techniques to optimize Python code, making it faster and more efficient. Whether you're a beginner or an experienced developer, these strategies will help you enhance the performance of your Python applications.  ## Understanding Performance Bottlenecks  Before diving into optimization techniques, it's essential to identify the performance bottlenecks in your code. Common areas include:  - **Inefficient algorithms**: Using suboptimal algorithms can significantly slow down your application. - **Excessive memory usage**: High memory consumption can lead to slower execution times. - **I/O operations**: Reading from or writing to files and databases can be time-consuming. - **Network latency**: Delays in network communication can impact performance.  ## Techniques for Performance Optimization  ### 1. Profiling Your Code  Profiling is the first step in performance optimization. It helps identify the parts of your code that consume the most time and resources. Python provides several profiling tools, such as:  - **cProfile**: A built-in module that provides a detailed report of function calls. - **line_profiler**: A third-party tool that offers line-by-line profiling.  #### Example: Using cProfile  ```python import cProfile  def my_function():     # Your code here     pass  cProfile.run('my_function()') ```  ### 2. Using Efficient Data Structures  Choosing the right data structure can have a significant impact on performance. Consider using:  - **Lists** for dynamic arrays. - **Dictionaries** for fast lookups. - **Sets** for unique elements and membership tests.  #### Example: Dictionary vs. List  ```python # Using a list items = ['apple', 'banana', 'cherry'] if 'banana' in items:     print('Found!')  # Using a dictionary items = {'apple': True, 'banana': True, 'cherry': True} if 'banana' in items:     print('Found!') ```  ### 3. Leveraging Built-in Functions  Python's built-in functions are implemented in C and are highly optimized. Use functions like `sum()`, `min()`, `max()`, and `sorted()` for better performance.  #### Example: Using sum()  ```python numbers = [1, 2, 3, 4, 5] total = sum(numbers) ```  ### 4. Avoiding Global Variables  Global variables can slow down your code due to the need for global lookups. Use local variables within functions to improve speed.  ### 5. Using List Comprehensions  List comprehensions are more efficient than traditional loops for creating lists. They are concise and often faster.  ```python # Traditional loop squares = [] for i in range(10):     squares.append(i * i)  # List comprehension squares = [i * i for i in range(10)] ```  ### 6. Implementing Caching  Caching can significantly reduce computation time by storing the results of expensive function calls. Use the `functools.lru_cache` decorator to cache function outputs.  ```python from functools import lru_cache  @lru_cache(maxsize=None) def fibonacci(n):     if n < 2:         return n     return fibonacci(n-1) + fibonacci(n-2) ```  ### 7. Parallelizing Code  Take advantage of multi-core processors by parallelizing your code. Use the `concurrent.futures` module for simple parallel execution.  ```python from concurrent.futures import ThreadPoolExecutor  def process_data(data):     # Process data     pass  with ThreadPoolExecutor() as executor:     executor.map(process_data, data_list) ```  ### 8. Using External Libraries  For computationally intensive tasks, consider using external libraries like NumPy and Pandas, which are optimized for performance.  #### Example: Using NumPy  ```python import numpy as np  # Create a large array array = np.arange(1000000)  # Perform operations result = np.sum(array) ```  ## Advanced Topics  ### Memory Management  Efficient memory management is crucial for optimizing performance. Python provides several tools and techniques for managing memory usage:  - **Garbage Collection**: Python's garbage collector automatically manages memory allocation and deallocation. Use the `gc` module to control garbage collection. - **Memory Profiling**: Use tools like `memory_profiler` to analyze memory usage and identify memory leaks.  ### Concurrency and Parallelism  Python supports concurrency and parallelism through various modules and libraries:  - **Threading**: Use the `threading` module for concurrent execution of tasks. - **Multiprocessing**: Use the `multiprocessing` module for parallel execution of tasks across multiple CPU cores.  ### Profiling Tools  In addition to `cProfile` and `line_profiler`, consider using:  - **Py-Spy**: A sampling profiler for Python that provides real-time performance insights. - **Scalene**: A high-performance, high-precision CPU, GPU, and memory "},{"slug":"performance-optimization-tricks","title":"Performance Optimization Tricks and Tips With EF Core & .NET 8","description":"Learn performance optimization techniques for Entity Framework Core with .NET 7 using real-time examples.","tags":["Entity Framework Core","Performance Optimization"],"pubDate":"2023-05-23","content":" **Performance optimization is a critical aspect of developing robust and efficient applications, especially when working with a data access framework like Entity Framework Core (EF Core) in conjunction with the powerful .NET 8 platform.**  In this article, we will explore some performance optimization tricks and tips with EF Core and demonstrate their application in a real-time example using an accounting application scenario.  Before diving into the optimization techniques, let's briefly understand EF Core and its relevance in the .NET ecosystem. EF Core is a lightweight, extensible, and cross-platform version of Entity Framework, a popular Object-Relational Mapping (ORM) framework for .NET. It simplifies data access by providing a high-level API for querying and manipulating databases.  Now, let's consider an accounting application scenario where we have entities such as `Account`, `Transaction`, and `User`. Each Account can have multiple Transaction records associated with it, and each Transaction is linked to a User. Our goal is to optimize the performance of data retrieval and manipulation operations in this scenario.  ## 1. Use AsNoTracking()  One of the simplest yet effective techniques to improve performance is using the `AsNoTracking()` method. By default, EF Core tracks changes to entities in memory for change tracking purposes. However, if you only need to read data without modifying it, calling `AsNoTracking()` eliminates the overhead of change tracking. For example, when fetching a list of accounts, you can use:   var accounts = context.Accounts.AsNoTracking().ToList();   ## 2. Select Required Columns  Fetching all columns of an entity when you only need a subset of them can impact performance, especially when dealing with large datasets. Instead, use the `Select` method to fetch only the required columns. For instance, if you need the account name and balance, you can write:  ```csharp var accounts = context.Accounts.Select(a => new { a.Name, a.Balance }).ToList(); ```  ## 3. Paging with Skip and Take  When dealing with large datasets, fetching all records at once can lead to performance issues and increased memory consumption. Instead, use paging techniques with `Skip` and `Take` to fetch data in smaller chunks. This approach is particularly useful for scenarios where you need to display data in paginated views. For example:   var pageSize = 10; var pageNumber = 1; var transactions = dbContext.Transactions     .OrderByDescending(t => t.TransactionDate)     .Skip((pageNumber - 1) * pageSize)     .Take(pageSize)     .ToList();   ## 4. Eager Loading with Include  By default, EF Core uses lazy loading, which means related entities are loaded on-demand when accessed. However, lazy loading can result in the N+1 query problem, where a separate query is executed for each entity, leading to performance degradation. To mitigate this, use eager loading with the `Include` method to fetch related entities upfront. For example:   var accounts = dbContext.Accounts     .Include(a => a.Transactions)     .ToList();   ## 5. Raw SQL Queries  In certain scenarios, when complex queries or performance optimization is crucial, EF Core allows executing raw SQL queries. Raw SQL queries provide full control over the query structure and can be highly efficient. However, be cautious about SQL injection vulnerabilities and ensure proper parameterization. Here's an example:   var accountId = 1; var query = @\"SELECT * FROM Transactions WHERE AccountId = {0}\"; var transactions = dbContext.Transactions.FromSqlRaw(query, accountId).ToList();   These performance optimization techniques can significantly enhance the speed and efficiency of your accounting application. However, it's essential to measure and profile the performance improvements to ensure they are effective. Now, let's explore additional tips and best practices to optimize EF Core performance in the context of our accounting application scenario.  ## 6. Indexing  Indexing plays a crucial role in enhancing query performance. Analyze the queries executed against your database and identify frequently accessed columns. Then, create appropriate indexes to speed up the retrieval of data. In our scenario, you might consider indexing columns like `AccountId` in the `Transactions` table for faster filtering based on account.  ## 7. Batch Updates and Inserts  When dealing with bulk updates or inserts, EF Core can incur significant performance overhead due to individual database round-trips for each entity. To optimize this, EF Core provides the `AddRange` and `UpdateRange` methods to perform batch operations, reducing the number of round-trips. For example:   ```csharp var transactions = new List { /* list of transactions */ }; dbContext.Transactions.AddRange(transactions); dbContext.SaveChanges(); ```   ## 8. Use Stored Procedures  EF Core supports executing stored procedures using the `FromSqlRaw` method. Stored procedures can be highly efficient and can be used for compl"},{"slug":"profile","title":"My Profile","description":"Professional profile of M. F.M Fazrin, a Software Development Specialist with extensive experience in .NET, Azure, and full-stack development.","tags":["Mohamed","Fazrin","Farook","Software","Engineer"],"pubDate":"2022-04-30","content":"   # My Profile  **M.F.M Fazrin (MSc – SE) - 📌 Doha, Qatar.**  📞[+97433253203](tel:+97433253203)  📱 [+94772049123](https://wa.me/94772049123)  ✉️ [info@dotnetevangelist.net](mailto:info@dotnetevangelist.net)  🤵🏻 [https://nirzaf.github.io](https://nirzaf.github.io/)  🐱‍👤[https://github.com/nirzaf](https://github.com/nirzaf)  💼[https://linkedin.com/in/mfmfazrin](https://linkedin.com/in/mfmfazrin)  📝[https://dotnetevangelist.net](https://dotnetevangelist.net/)  ⭐[Highest-ranked developer in Qatar](https://stardev.io/top/developers/all/in/qatar)        ---  ## Professional Experience  ### Software Development Specialist (July-2022) @ Primary Health Care Corporation (Qatar) - Project: Nar'aakom Mobile Application (Backend with Azure Functions with C#) - Working on backend .NET services to migrate REST API to GraphQL with Azure Active Directory. - Optimizing the query performance using Azure Redis distributed caching. - Migrating data from SQL server to FHIR (NoSql type) database - Developing an Open API interface to allow third-party services to integrate with the Nar'aakom application - Unit testing and Integration testing with [xUnit.net](http://xunit.net/) and Moq framework. - Tools & Technologies Using - .NET 6 & above, C# 10 & above, Azure Functions, Visual Studio 2022, Jet brains Rider, [xUnit.net](http://xunit.net/), ReSharper, SQL Server 2022, CI CD pipeline in Azure DevOps.  ### Senior Full-stack Engineer (July-2020 – June-2022) @ Quadrate Tech Solutions Private Limited - Project: Hotel ERP (SaaS-based ERP Solution for hotels) - Working in the administration module, which handles authentication & authorization along with primary configurations of each property's subscribed modules - Successfully deployed a mail service and SMS gateway using Azure Functions and Logic Apps to communicate with users and third parties. - Synchronize legacy data from SQL Server to Cosmos Database via SQL API to increase the scalability of the system - Contributing to user interface development using Angular - Microservices integration using Azure Service Bus & RabbitMQ (Pub-Sub pattern) - Configure CI-CD using Azure DevOps pipelines - Unit Testing with [xUnit.net](http://xunit.net/) (Mock, [Faker.Net](http://faker.net/), Stubs) - Tools & Technologies Using - .NET Core 3.1 & above, C# 9 & above, Angular 12, Azure Functions, Azure Logic Apps, Cosmos Db, VS 2022, VS Code, Jet brains Rider, [xUnit.net](http://xunit.net/), ReSharper, Azure Service Bus, RabbitMQ, SQL Server 2022, CI CD pipeline in Azure DevOps.  ### Dot NET Engineer (Nov 2019 - June 2020) @ Voigue Private Limited - Project: SmartPABX - Cloud-Based Phone System - .NET Engineer in WPF-based Project for an Australian Telecommunication Company. - Developed Backend API with .NET Core and Updated the existing PABX legacy system to the latest version. - User interface optimized completely by converting WinForms to WPF and enabled Dynamic User interface functions such as call forward, group calls, call parking and many other features. - Tools & Technologies Used - .NET Core 2.1, AsterNET, WPF, C# (7), REST API, XAML, PABX Asterisk, MariaDB, Apache Server, JSON, Visual Studio 2019.  ### Associate Full Stack-Engineer (01/2019 - 10/2019) @ Virtusa Private Limited - McDonald's - Workforce Operations Labor Forecasting System - Developed a web application to forecast labour requirements for various restaurant configurations based on frequently migrated sales data to improve labour allocations. The Decision-making process improved by five minutes versus the previous six months and zeroed the manager person-hours needed to input and cleanse the sales data. - Implementation of new features. Optimizations. Bug prioritizing and bug fixing and producing detailed technical documentation. - 10/10 client scorecards from the project's inception till production were maintained. - Technologies Used - Angular 6, Bootstrap 4.2, .NET Core 2.0, Web API, EF Core 2.0, AutoMapper, SQL Server 2017, Azure Web Jobs, Azure Blob and File Storage, Azure Insights, Azure Redis Cache, Visual Studio 2017, Azure CI/CD and OpenXml.  ### Software Developer (Jan 2016 - Dec 2018) @ Nemico Holdings - Inventory Management System for large fashion retailers - POS integration to keep updating of inventory database with day-to-day transaction - Successfully fixed bugs as per the Agile methodology implemented new user privileges, retrofitted the legacy product management tool, and implemented a cloud-based enterprise inventory management developed in [ASP.NET](http://asp.net/) (4.8) - Successfully migrated the complete reporting system from Crystal Reports to iTextSharp to improve the performance and save cost from the enterprise reporting tool - Tools & Technologies Used - C#.NET, Microsoft Visual Studio 2015, Microsoft SQL Server, 2012, [ASP.NET](http://asp.net/), iTextSharp, Crystal Report for Web.  ### Backend Developer (06/2012 - 07/2015) @ Olayan (Saudi Arabia) - Centralize the online order-taking proc"},{"slug":"react-tutorial-with-vite-week-02","title":"How To Install React 19 With Vite","description":"A fast, beginner-friendly tutorial for setting up React 19 with Vite, including tips, troubleshooting, and best practices.","tags":[],"pubDate":"May 24 2025","content":" React 19 is the latest stable version of the popular JavaScript library for building user interfaces. Vite is a modern build tool that offers blazing-fast development and optimized production builds. In this tutorial, you'll learn how to set up React 19 with Vite to create a fast, modern web application environment.  ### Why Use Vite & React 19?          ⚡ Why Use Vite?            Lightning-fast hot module replacement       No bundling during development       Optimized builds with Rollup       Out-of-the-box TypeScript support                ⚛️ React 19 Features            React Compiler for improved performance       New server-side rendering architecture       Enhanced concurrent features       Improved developer experience           ### Prerequisites  Before getting started, make sure you have:  - **Node.js** (v18.0.0 or higher, v20+ recommended) - **npm** or **yarn** - A code editor (VS Code recommended)  > **Tip:** To check your Node.js version, run: > > ```bash > node -v > ```  ### Step-by-Step Installation  #### 1. Create a New Vite Project  Open your terminal and run:  ```bash npm create vite@latest my-react-app -- --template react ```  Replace `my-react-app` with your desired project name.  #### 2. Navigate to Your Project Directory  ```bash cd my-react-app ```  #### 3. Upgrade to React 19  By default, Vite may create a project with React 18. Upgrade to React 19:  ```bash npm install react@^19.0.0 react-dom@^19.0.0 ```  > **Note:** In the future, Vite's React template might include React 19 by default.  #### 4. Install React Compiler (Optional)  To leverage the new React Compiler features in React 19, install the Babel plugin:  ```bash npm install -D @babel/plugin-react-compiler ```  #### 5. Configure Vite to Use React Compiler  Open your `vite.config.js` and add the plugin:  ```js    export default defineConfig({   plugins: [     react({       babel: {         plugins: ['@babel/plugin-react-compiler']       }     })   ], }); ```  #### 6. Change Default Port (Optional)  To use port 3000 instead of 5173, modify `vite.config.js`:  ```js    export default defineConfig({   plugins: [     react({       babel: {         plugins: ['@babel/plugin-react-compiler']       }     })   ],   server: {     port: 3000   }, }); ```  #### 7. Install Dependencies and Start the Dev Server  ```bash npm install npm run dev ```  Your React 19 app should now be running on `http://localhost:3000` (or port 5173 if unchanged).  ### Verifying React 19 Installation  To confirm your project uses React 19, display the React version in your app:  ```jsx // In App.jsx   function App() {   return (            React 19 with Vite       Current React Version: {version}        ); }   ```  This will show the current React version in your app, confirming React 19 is installed.  ### Troubleshooting  #### Package Version Conflicts  If you encounter version conflicts, clear your npm cache and reinstall:  ```bash npm cache clean --force npm install ```  #### Compiler Plugin Issues  If the React Compiler plugin causes issues, you can remove it and still use React 19 without compiler features:  1. Remove the plugin from `vite.config.js` 2. Uninstall the package:    ```bash    npm uninstall @babel/plugin-react-compiler    ```  ### Next Steps          Add Dependencies     Consider adding these popular libraries:            React Router for navigation       Tailwind CSS for styling       Axios for API requests                Project Structure     Organize your project with these folders:            src/components/ - Reusable components       src/pages/ - Page components       src/hooks/ - Custom React hooks       src/context/ - Context providers           > **Pro Tip:** Explore React 19's new features like React Compiler, improved Suspense, and server components to make the most of your new setup!  "},{"slug":"sending-automated-emails-from-azure-logic-apps","title":"Sending Emails From Azure Logic Apps Service Bus","description":"Learn how to send emails from Azure Logic Apps triggered by Azure Service Bus messages.","tags":["Azure","Logic Apps","Service Bus","Email"],"pubDate":"2024-02-28","content":" **Automate sending emails based on Azure Service Bus messages using Azure Logic Apps.**  ## Introduction  This blog post demonstrates how to configure Azure Logic Apps to send emails triggered by messages received on an Azure Service Bus topic subscription.  This is a powerful way to automate notifications and other email-driven processes based on events captured in your Service Bus.  ## Prerequisites  - Azure Subscription - Email provider (Such as SendGrid, MailGun, Postmark, etc.)  ## Background  ### What is Azure Logic Apps?  Azure Logic Apps is a cloud-based service for creating and running automated workflows that integrate apps, data, systems, and services across enterprises or organizations. It simplifies the design and development of scalable solutions for integration scenarios.  ### What is Azure Service Bus?  Azure Service Bus is a fully managed message broker used for reliable asynchronous communication between applications and services. It decouples systems and enables secure data transfer using messages.  ### What is an Email Provider?  An email provider offers email hosting services. It's distinct from an email client (like Outlook or Gmail), which is the software used to access email.  You'll need an email provider to send emails from your Logic App.  ## Sending Emails from Logic Apps  Sending emails through Azure Logic Apps simplifies fulfilling business requirements, especially when triggered by Azure Service Bus.  The Azure Service Bus message hierarchy is:  - Service Bus Namespace - Topic - Subscriptions  ## Implementation Steps  1. **Create a Resource Group and a Logic App:**  Start by creating a resource group and a new Logic App within the Azure portal.  2. **Configure the Trigger:** In the Logic App Designer, select the \"When a message is received in a topic subscription\" trigger. content_copy Use code with caution. Mdx  When a message is available in a topic subscription        ![Logic App Trigger](https://ik.imagekit.io/quadrate/assets/img/dotnetevangelist/Azure_Logic_Apps_Email.jpg?updatedAt=1746813307747)     Provide the connection string for your Service Bus, the topic name, and the subscription name.  **Sample ARM template for the trigger:**  ```json {     \"inputs\": {         \"parameters\": {             \"topicName\": \"your-topic-name-here\",             \"subscriptionName\": \"your-subscription-name-here\"         },         \"serviceProviderConfiguration\": {             \"connectionName\": \"serviceBus\",             \"operationId\": \"receiveTopicMessages\",             \"serviceProviderId\": \"/serviceProviders/serviceBus\"         }     },     \"splitOn\": \"@triggerOutputs()?['body']\" }  ```  ### Create an HTTP Action  Add an HTTP action to send the email.  - **Method:** POST - **URI:** Your email provider's POST API endpoint URI - **Headers:** Include necessary authentication tokens and content-type headers. - **Body:** Construct the email body using data from the Service Bus message. The following example shows how to access properties from the message:  #### Sample ARM template for the HTTP action:  ```json {     \"inputs\": {         \"method\": \"POST\",         \"uri\": \"your-email-service-provider-API-endpoint\",         \"headers\": {             \"token\": \"your-authentication-token\",             \"content-type\": \"application/json\"         },         \"body\": {             \"From\": \"@triggerOutputs()?['body']?['UserProperties/from']\",             \"HtmlBody\": \"@triggerOutputs()?['body']?['UserProperties/htmlBody']\",             \"ReplyTo\": \"@triggerOutputs()?['body']?['replyTo']\",             \"Subject\": \"@triggerOutputs()?['body']?['UserProperties/subject']\",             \"To\": \"@triggerOutputs()?['body']?['to']\"         }     } } ``` **Save the Workflow:** Save the Logic App workflow. It will now trigger every time a message is received on the specified Service Bus topic subscription. "},{"slug":"sidecar-pattern-with-examples-in-asp.net-core","title":"Sidecar Pattern With Examples in Asp.NET Core","description":"Exploring the Sidecar Pattern with practical examples in ASP.NET Core","tags":["Sidecar","Asp.net Core","Architecture"],"pubDate":"2024-02-29","content":" **The Sidecar pattern is a powerful architectural approach that enhances modularity and flexibility in ASP.NET Core applications.**  ## Introduction  In the world of microservices and distributed systems, the Sidecar pattern has emerged as a valuable architectural solution. This blog post explores the Sidecar pattern, its implementation in ASP.NET Core, and provides practical examples to illustrate its benefits and potential drawbacks.  ## What is the Sidecar Pattern?  The Sidecar pattern involves splitting an application into two separate processes: 1. A primary application 2. A \"sidecar\" process  The sidecar process runs alongside the main application, providing additional functionality such as caching, logging, monitoring, or authentication.  ![Sidecar pattern](https://ik.imagekit.io/quadrate/assets/img/dotnetevangelist/side-car-pattern.jpg?updatedAt=1746813294385)  ## Implementing the Sidecar Pattern in ASP.NET Core  In ASP.NET Core, the Sidecar pattern can be implemented using middleware. Middleware intercepts requests and responses, allowing for additional processing between the web server and the application.  ### Pros of the Sidecar Pattern  - Separation of concerns - Re-usability - Isolation - Customization  ### Cons of the Sidecar Pattern  - Increased complexity - Overhead - Coordination challenges - Debugging difficulties  ## Example: Sidecar Pattern with Middleware in ASP.NET Core  Here's a basic example of implementing the Sidecar pattern using middleware:   ```csharp // Startup.cs public void Configure(IApplicationBuilder app, IWebHostEnvironment env) {     app.UseSidecar();     app.UseRouting();     app.UseEndpoints(endpoints =>     {         endpoints.MapControllers();     }); }  // SidecarMiddleware.cs public class SidecarMiddleware {     private readonly RequestDelegate _next;     private readonly ILogger _logger;      public SidecarMiddleware(RequestDelegate next, ILogger logger)     {         _next = next;         _logger = logger;     }      public async Task Invoke(HttpContext context)     {         _logger.LogInformation($\"Processing request: {context.Request.Path}\");         await _next(context);     } }  // SidecarMiddlewareExtensions.cs public static class SidecarMiddlewareExtensions {     public static IApplicationBuilder UseSidecar(this IApplicationBuilder builder)     {         return builder.UseMiddleware();     } } ```  ## Advanced Example: Sidecar Pattern for Authentication  Let's explore a more complex example using the Sidecar pattern for authentication:  1. Create an ASP.NET Core API project 2. Add a Docker Compose project 3. Create a Dockerfile for the sidecar container 4. Configure authentication and authorization middleware 5. Set up Docker profiles 6. Define the docker-compose.yml file  Here's a snippet of the docker-compose.yml file:  ```yaml version: '3.9' services:   myapi:     build:       context: ./MyApi       dockerfile: Dockerfile     environment:       - ASPNETCORE_ENVIRONMENT=Development       - ASPNETCORE_URLS=http://0.0.0.0:80       - ServiceUrl=http://0.0.0.0:80       - AuthUrl=http://authsidecar:80     ports:       - \"8080:80\"     depends_on:       - authsidecar     networks:       - mynetwork    authsidecar:     build:       context: ./AuthSidecar       dockerfile: Dockerfile     environment:       - ASPNETCORE_ENVIRONMENT=Development     ports:       - \"8081:80\"     networks:       - mynetwork  networks:   mynetwork: ```  ## Ambassador Pattern: A Variation of the Sidecar Pattern  The Ambassador pattern is a variation that uses an ambassador container to handle communication between the primary container and external services. Here's a basic example:  ```csharp // AmbassadorMiddleware.cs public class AmbassadorMiddleware {     private readonly RequestDelegate _next;     private readonly IHttpClientFactory _httpClientFactory;      public AmbassadorMiddleware(RequestDelegate next, IHttpClientFactory httpClientFactory)     {         _next = next;         _httpClientFactory = httpClientFactory;     }      public async Task Invoke(HttpContext context)     {         var client = _httpClientFactory.CreateClient();         var response = await client.GetAsync(\"http://localhost:8080/api/values\");         var content = await response.Content.ReadAsStringAsync();         await context.Response.WriteAsync(content);     } } ```  The Sidecar pattern in ASP.NET Core provides a flexible and modular architecture that allows developers to add and remove functionality as needed, without impacting the main application. While it offers numerous benefits, it's important to consider the potential drawbacks and use the pattern judiciously in your projects. --- "},{"slug":"symbiotic-workflows-engineering-ai-co-pilots","title":"Engineering AI Co-Pilots for Executive Decision Support","description":"Learn how to design and integrate bespoke AI agents that adapt to executive communication styles, decision patterns, and information priorities for enhanced strategic decision-making.","tags":["Ai","Executive Tools","Decision Support","Personalization","Data Security"],"pubDate":"2023-05-09","content":" # Symbiotic Workflows: Engineering AI Co-Pilots for Hyper-Personalized Executive Decision Support  **The C-suite of 2026 will rely on AI systems that go beyond generic productivity tools. This tutorial explores how to create truly personalized AI co-pilots that adapt to individual executive styles and needs.**  ## Introduction: Beyond Generic AI Assistants  Today's executives face unprecedented complexity in decision-making. The volume of data, speed of market changes, and intricacy of global operations demand cognitive capabilities that even the most talented leaders struggle to maintain consistently.  Generic AI assistants offer limited value in this context. True executive augmentation requires systems that:  1. Deeply understand individual communication preferences 2. Learn decision-making patterns specific to the executive 3. Prioritize information based on personal and organizational values 4. Maintain absolute security and confidentiality 5. Adapt continuously to changing priorities  This tutorial will guide you through creating such systems - what we call **Executive AI Co-pilots**.  ## The Architecture of Executive AI Co-pilots  Before diving into implementation, let's understand the architecture of an effective executive co-pilot system:        ### Key Components Explained:  1. **Executive Interaction Layer**: The interfaces through which executives interact with the system 2. **Personalization Engine**: The heart of the co-pilot, containing models of executive preferences 3. **Core AI Models**: Foundation models and specialized algorithms for different tasks 4. **Knowledge Integration Layer**: Connects to various data sources while maintaining context 5. **Data Sources**: Internal and external information repositories 6. **Security & Compliance Framework**: Ensures data protection and regulatory adherence 7. **Continuous Learning Loop**: Enables the system to improve through usage  ## Step 1: Building the Executive Profile  The foundation of personalization is a comprehensive executive profile. This goes far beyond basic preferences to capture cognitive patterns and decision frameworks.  ### Data Collection Strategy  ```mermaid graph TD     A[Executive Profile] --> B[Communication Style]     A --> C[Decision Patterns]     A --> D[Information Priorities]     A --> E[Risk Tolerance]     A --> F[Time Horizons]      B --> G[Email Analysis]     B --> H[Meeting Transcripts]     B --> I[Document Edits]      C --> J[Historical Decisions]     C --> K[Decision Speed]     C --> L[Influencing Factors]      D --> M[Attention Patterns]     D --> N[Information Sources]     D --> O[Detail Preference]      classDef primary fill:#f9f,stroke:#333,stroke-width:2px     classDef secondary fill:#bbf,stroke:#333,stroke-width:1px     class A primary     class B,C,D,E,F secondary ```  ### Implementation Code: Profile Generator  ```python class ExecutiveProfiler:     def __init__(self, executive_id, security_level=\"maximum\"):         self.executive_id = executive_id         self.security_level = security_level         self.profile = self._initialize_profile()         self.data_collectors = self._setup_collectors()      def _initialize_profile(self):         \"\"\"Create empty profile structure with all necessary categories\"\"\"         return {             \"communication\": {                 \"formality_level\": None,                 \"preferred_length\": None,                 \"vocabulary_complexity\": None,                 \"response_time_preference\": None             },             \"decision_making\": {                 \"risk_tolerance\": None,                 \"data_dependency\": None,                 \"collaborative_tendency\": None,                 \"decision_speed\": None             },             \"information_processing\": {                 \"detail_orientation\": None,                 \"visual_vs_textual\": None,                 \"preferred_data_formats\": [],                 \"context_requirements\": None             }         }      def _setup_collectors(self):         \"\"\"Initialize data collectors based on security level\"\"\"         collectors = []         if self.security_level == \"maximum\":             # On-premise collectors only             collectors.append(EmailAnalyzer(on_premise=True))             collectors.append(MeetingAnalyzer(on_premise=True))         else:             # Could include cloud services             collectors.append(EmailAnalyzer(on_premise=False))             collectors.append(MeetingAnalyzer(on_premise=False))          return collectors      def generate_initial_profile(self, data_sources):         \"\"\"Generate initial profile from historical data\"\"\"         for collector in self.data_collectors:             profile_updates = collector.analyze(data_sources)             self._update_profile(profile_updates)          return self.profile      def _update_profile(self, updates):         \"\"\"Update profile with new insights\"\"\"         # Implementation of profile updating logic         pass ```  ## Step 2: Designing the "},{"slug":"the-new-extensions-everything-feature-of-csharp-13","title":"The New Extensions Everything Feature of C# 13","description":"A detailed look at the new Extensions Everything feature in C# 13","tags":["C#","Programming",".Net"],"pubDate":"2024-09-28","content":" Let's delve deeper into the new Extensions feature in C# 13 and explore how it can be a significant change for code readability and functionality. We'll look at each aspect of Extensions in detail and illustrate them with real-world examples and coding samples.  ## Implicit vs. Explicit Extensions  Introduce this section with a brief description or an overview.  ### Implicit Extensions  Implicit extensions in C# 13 allow you to extend a class without requiring any special syntax when using them. This means that you can use the extended functionality just like you would use any other member of the class.  ```csharp public class Product {     public decimal Price { get; set; } }  public extension ProductExtensions extends Product {     public decimal CalculateSalesTax(decimal taxRate)     {         return Price * taxRate;     } } ```  ### Explicit Extensions  Explicit extensions enable you to create projections or subcategories of the original class. This can be useful when you want to add conditional methods or properties based on the extended type.  ```csharp public extension TaxableProduct extends Product {     public decimal CalculateSalesTax(decimal taxRate)     {         return Price * taxRate;     } } ```  ## Instance Members  One of the major improvements in C# 13's Extensions is the ability to include instance members (properties and methods) in addition to static methods. This allows for a more natural syntax when using the extended functionality.  ```csharp var product = new Product { Price = 100 }; var salesTax = product.CalculateSalesTax(0.05m); ```  ## Improved Code Organization  By combining implicit and explicit extensions in a single class, you can organize the extension logic alongside the code that uses it. This enhances readability and maintainability by keeping related code together.  ## Conclusion  The Extensions feature in C# 13 represents a significant addition to the language that promises to improve code readability, maintainability, and expressiveness. By allowing developers to add functionality to existing classes without modifying their original code, Extensions promote code reuse and reduce the need to modify original class definitions.  Thanks to Nick Chapsas  The New Extensions EVERYTHING Feature of C# 13! — YouTubeThe New Extensions EVERYTHING Feature of C# 13! — YouTube "},{"slug":"the-state-of-the-developer-ecosystem-2024","title":"The State of the Developer Ecosystem 2024: A Comprehensive Analysis","description":"An in-depth analysis of the programming world, examining the tools, languages, and technologies developers use today.","tags":["Developer","Ecosystem","Programming","Trends","2024"],"pubDate":"2024-12-12","content":" The JetBrains 2024 State of Developer Ecosystem [Report](https://www.jetbrains.com/lp/devecosystem-2024) provides an in-depth analysis of the programming world, examining the tools, languages, and technologies developers use today. Surveying over 23,000 developers globally, this report offers insights into current trends, emerging technologies, and preferences within the software development community. Here, we dissect every detail, categorized for clarity.  ## Key Takeaways from the Report    ## Programming Languages  ### Top 10 Programming Languages in 2024  #### JavaScript  - **Adoption:** 61% of developers continue to use JavaScript, especially in web development. - **Trends:** Although still dominant, its use has slightly decreased as alternatives gain traction. - **Strengths:** Ubiquity, large ecosystem (NPM), and ease of integration with various platforms. - **Challenges:** Managing large codebases, runtime performance issues, and inconsistencies across environments.     #### Python  - **Adoption:** A favorite for 47% of developers, especially in data science, machine learning, and automation. - **Trends:** A steady rise due to its versatility and ease of learning for beginners. - **Strengths:** Readability, extensive libraries (NumPy, Pandas, TensorFlow), and strong community support. - **Challenges:** Performance limitations in computationally-heavy tasks, dependency conflicts.  #### TypeScript  - **Adoption:** Used by 35% of developers, showing a significant rise since 2017 (12%). - **Trends:** Preferred by JavaScript developers for its type-safety and maintainability. - **Strengths:** Static typing, early error detection, and integration with existing JavaScript projects. - **Challenges:** Learning curve for beginners, slower build times due to type-checking.  #### Java  - **Adoption:** A workhorse for 33% of developers, largely in enterprise and Android development. - **Trends:** Decline in adoption as Kotlin and modern alternatives emerge in specific areas. - **Strengths:** Platform independence, stability, and extensive library support. - **Challenges:** Verbosity and competition from lightweight languages like Kotlin.  #### C#  - **Adoption:** Preferred by 28% of developers, especially for Windows applications and game development. - **Trends:** Growth in popularity due to Unity’s dominance in game development. - **Strengths:** Strong integration with Microsoft ecosystems, modern syntax, and a thriving community. - **Challenges:** Platform dependency for non-Windows environments, slower adoption outside of the enterprise sector.  #### Rust  - **Adoption:** Used by 18% of developers, especially those seeking performance and safety. - **Trends:** Growth driven by the need for high-performance and memory-safe applications. - **Strengths:** Zero-cost abstractions, ownership model, and robust performance. - **Challenges:** Steep learning curve and limited library ecosystem compared to established languages.    #### Kotlin  - **Adoption:** A favorite for Android developers, with 16% usage overall. - **Trends:** Adoption accelerated by Google’s official endorsement for Android development. - **Strengths:** Concise syntax, null safety, and seamless interoperability with Java. - **Challenges:** Slower build times and smaller community compared to Java.  #### PHP  - **Adoption:** Used by 15% of developers, primarily in web development. - **Trends:** Steady decline due to competition from modern back-end frameworks like Node.js. - **Strengths:** Easy to deploy, vast library support, and widespread hosting availability. - **Challenges:** Security vulnerabilities and perception as an outdated language.  #### Go (Golang)  - **Adoption:** Employed by 12% of developers, particularly in cloud and DevOps projects. - **Trends:** Growth driven by simplicity and concurrency support. - **Strengths:** Lightweight, efficient performance, and excellent support for microservices. - **Challenges:** Limited third-party libraries and a smaller talent pool compared to Java or Python.  #### Swift  - **Adoption:** Used by 9% of developers, mainly for iOS/macOS development. - **Trends:** Steady usage, driven by Apple’s ecosystem requirements. - **Strengths:** Performance, safety features, and native support for Apple platforms. - **Challenges:** Platform dependency and limited use cases outside the Apple ecosystem.  ### Language Adoption by Category  #### Web Development  - JavaScript (61%), TypeScript (35%), and PHP (15%) dominate web development. - Frameworks like React and Angular drive JavaScript and TypeScript usage.    #### Mobile Development  - Kotlin (16%) for Android and Swift (9%) for iOS are the top choices. - Flutter’s rise is promoting Dart adoption, though it remains niche.  #### Data Science and AI  - Python (47%) leads by a wide margin, with R and Julia making minor inroads.  #### Game Development  - C# (28%) dominates, thanks to Unity, with C++ also seeing significant use in AAA game development.  #### Syst"},{"slug":"the-ultimate-guide-to-authentication-in-nextjs-15","title":"The Ultimate Guide to Authentication in Next.js 15","description":"A comprehensive guide to implementing authentication in Next.js 15 applications, covering popular libraries like Auth.js (NextAuth.js), Clerk, Supabase Auth, and custom solutions.","tags":["Next.js","Authentication","Next.js 15","Auth.js","NextAuth.js","Clerk","Supabase","Security","Web Development"],"pubDate":"2024-07-27","content":" Authentication is a critical component of modern web applications. With Next.js 15 continuing to build on the robust foundation of the App Router, Server Components, and Server Actions, choosing and implementing the right authentication strategy is more important than ever. This guide will walk you through various approaches to secure your Next.js 15 applications.  ## Key Considerations for Next.js 15 Authentication  Before diving into specific methods, consider these aspects relevant to Next.js 15:  *   **App Router:** Most modern Next.js apps use the App Router. Your authentication solution should integrate seamlessly with its conventions, particularly for protecting routes and accessing session data in Server Components and Route Handlers. *   **Server Components & Server Actions:** These are fundamental to Next.js 15. Authentication state needs to be accessible on the server without relying on client-side checks alone. Server Actions, especially, require robust authentication and authorization. *   **Security Best Practices:** Always prioritize security. This includes protection against CSRF, XSS, and ensuring secure session management. *   **Developer Experience:** Choose a solution that aligns with your team's skills and offers a smooth development workflow. *   **Scalability:** Consider how the solution will scale as your user base grows.  ## 1. Auth.js (Formerly NextAuth.js)  Auth.js is a highly popular, open-source authentication solution for Next.js and other frameworks. It's known for its flexibility and extensive list of OAuth providers.  **Pros:** *   **Extensive Provider Support:** Easily integrate with dozens of OAuth providers (Google, GitHub, Facebook, etc.), email/password, magic links, and credential-based login. *   **Highly Customizable:** Adaptable to various authentication flows and UI requirements. *   **Session Management:** Built-in session management, including JWTs and database sessions. *   **App Router Ready:** Designed to work well with the Next.js App Router. *   **Callbacks:** Provides callbacks for customizing behavior at different stages of the authentication process (e.g., `signIn`, `redirect`, `session`, `jwt`).  **Cons:** *   **Configuration Complexity:** Can be complex to set up for advanced scenarios due to its many options. *   **UI is BYO (Bring Your Own):** You'll need to build or integrate your own UI components for login pages, etc., though this offers maximum flexibility.  **High-Level Implementation (App Router):**  1.  **Installation:**     ```bash     npm install next-auth@beta @auth/core     ```     *(Note: `next-auth@beta` is often recommended for the latest Next.js features, ensure to check their docs for the most current stable version for Next.js 15)*  2.  **API Route Handler (`app/api/auth/[...nextauth]/route.ts`):**     ```typescript     // app/api/auth/[...nextauth]/route.ts     import NextAuth from 'next-auth'     import GitHubProvider from 'next-auth/providers/github'           export const authOptions = {       providers: [         GitHubProvider({           clientId: process.env.GITHUB_ID!,           clientSecret: process.env.GITHUB_SECRET!,         }),         CredentialsProvider({           name: \"Credentials\",           credentials: {             username: { label: \"Username\", type: \"text\" },             password: { label: \"Password\", type: \"password\" }           },           async authorize(credentials, req) {             // Add your logic here to look up the user from the credentials supplied             const user = { id: \"1\", name: \"J Smith\", email: \"jsmith@example.com\" } // Example              if (user) {               // Any object returned will be saved in `user` property of the JWT               return user             } else {               // If you return null then an error will be displayed advising the user to check their details.               return null               // You can also Reject this callback with an Error thus the user will be sent to the error page with the error message as a query parameter             }           }         })       ],       // secret: process.env.NEXTAUTH_SECRET, // Recommended for production       // pages: { // Optional: customize pages       //   signIn: '/auth/signin',       // }     }      const handler = NextAuth(authOptions)      export { handler as GET, handler as POST }     ```  3.  **Session Provider (`app/providers.tsx` or similar):**     ```tsx     // app/providers.tsx (or a similar client component)     'use client'     import { SessionProvider } from 'next-auth/react'      export default function Providers({ children }: { children: React.ReactNode }) {       return {children}     }     ```     And wrap your `layout.tsx`:     ```tsx     // app/layout.tsx     import Providers from './providers'      export default function RootLayout({ children }: { children: React.ReactNode }) {       return (                                 {children}                           )     }     ```  4.  "},{"slug":"using-supabase-postgresql-with-aspnet-core-and-entity-framework-core","title":"Using Supabase PostgreSQL with ASP.NET Core and Entity Framework Core","description":"Learn how to integrate Supabase PostgreSQL with ASP.NET Core and Entity Framework Core for a robust and scalable web application.","tags":["Supabase","Postgresql","Aspnetcore","Efcore","Database"],"pubDate":"2024-10-26","content":" **Supabase provides a powerful PostgreSQL database as a service that can be easily integrated with ASP.NET Core applications. This guide will walk you through setting up a complete integration between Supabase's PostgreSQL database and an ASP.NET Core web application using Entity Framework Core.**  ## Prerequisites  To follow along with this guide, you should have the following:  - .NET SDK 8.0 or later - A Supabase account and project - Basic understanding of ASP.NET Core and Entity Framework - Visual Studio 2022 or VS Code  ## Step 1: Project Setup  First, create a new ASP.NET Core Web API project using either Visual Studio or the .NET CLI:  ```bash dotnet new webapi -n SupabaseAspNetDemo cd SupabaseAspNetDemo ```  ### Install Required NuGet Packages  Add the following NuGet packages to your project:  ```xml             ```  You can install these packages using the Package Manager Console or .NET CLI:  ```bash dotnet add package Npgsql.EntityFrameworkCore.PostgreSQL dotnet add package Microsoft.EntityFrameworkCore.Design ```  ## Step 2: Database Context Setup  Create a new folder called `Data` and add a new class `ApplicationDbContext.cs`:  ```csharp public class ApplicationDbContext : DbContext {     public ApplicationDbContext(DbContextOptions options)         : base(options)     { }      public DbSet Todos { get; set; }      protected override void OnModelCreating(ModelBuilder modelBuilder)     {         base.OnModelCreating(modelBuilder);          modelBuilder.HasDefaultSchema(\"public\");          modelBuilder.Entity(entity =>         {             entity.ToTable(\"todos\");             entity.HasKey(e => e.Id);             entity.Property(e => e.Title).IsRequired();         });     } } ```  ## Step 3: Create Model Classes  Add a new folder called `Models` and create your entity classes:  ```csharp public class Todo {     public int Id { get; set; }     public string Title { get; set; }     public bool IsComplete { get; set; }     public DateTime CreatedAt { get; set; } } ```  ## Step 4: Configure Database Connection  ### Update appsettings.json  Add your Supabase database connection string to `appsettings.json`:  ```json {   \"ConnectionStrings\": {     \"DefaultConnection\": \"Host=db.your-project-ref.supabase.co;Database=postgres;Username=postgres;Password=your-password;Port=5432;SSL Mode=Require;Trust Server Certificate=true\"   } } ```  ### Configure Services  Update your `Program.cs` to register the DbContext:  ```csharp var builder = WebApplication.CreateBuilder(args);  builder.Services.AddDbContext(options => {     options.UseNpgsql(builder.Configuration.GetConnectionString(\"DefaultConnection\")); });  // Add other necessary services builder.Services.AddControllers(); builder.Services.AddEndpointsApiExplorer(); builder.Services.AddSwaggerGen();  var app = builder.Build(); ```  ## Step 5: Create API Controllers  Add a new controller to handle CRUD operations:  ```csharp [ApiController] [Route(\"api/[controller]\")] public class TodosController : ControllerBase {     private readonly ApplicationDbContext _context;      public TodosController(ApplicationDbContext context)     {         _context = context;     }      [HttpGet]     public async Task>> GetTodos()     {         return await _context.Todos.ToListAsync();     }      [HttpGet(\"{id}\")]     public async Task> GetTodo(int id)     {         var todo = await _context.Todos.FindAsync(id);         if (todo == null)         {             return NotFound();         }         return todo;     }      [HttpPost]     public async Task> CreateTodo(Todo todo)     {         _context.Todos.Add(todo);         await _context.SaveChangesAsync();         return CreatedAtAction(nameof(GetTodo), new { id = todo.Id }, todo);     }      [HttpPut(\"{id}\")]     public async Task UpdateTodo(int id, Todo todo)     {         if (id != todo.Id)         {             return BadRequest();         }          _context.Entry(todo).State = EntityState.Modified;         await _context.SaveChangesAsync();         return NoContent();     }      [HttpDelete(\"{id}\")]     public async Task DeleteTodo(int id)     {         var todo = await _context.Todos.FindAsync(id);         if (todo == null)         {             return NotFound();         }          _context.Todos.Remove(todo);         await _context.SaveChangesAsync();         return NoContent();     } } ```  ## Step 6: Database Migrations  Create and apply your initial migration:  ```bash # Using .NET CLI dotnet ef migrations add InitialCreate dotnet ef database update  # Or using Package Manager Console in Visual Studio Add-Migration InitialCreate Update-Database ```  ## Security Considerations  ### Protecting Connection Strings  In development, use User Secrets to store your connection string:  ```bash dotnet user-secrets set \"ConnectionStrings:DefaultConnection\" \"your-connection-string\" ```  For production, use environment variables or a secure configuration management system.  ### SSL Configuration  Ensure SSL is enabled for databa"},{"slug":"webassembly-takes-serverless-by-storm-boosting-performance-expanding-use-cases","title":"WebAssembly (Wasm) Takes Serverless by Storm: Boosting Performance and Expanding Use Cases","description":"Discover how WebAssembly is revolutionizing serverless computing, delivering near-native performance, unlocking new languages, and expanding the possibilities for cloud-native applications.","tags":["webassembly","wasm","serverless","cloud","performance"],"pubDate":"2025-05-13","content":" Serverless computing has transformed the way we build and deploy applications, offering scalability and cost efficiency. But as developers push the boundaries of what serverless can do, new challenges arise—especially around performance, language support, and cold start times. Enter WebAssembly (Wasm): a technology originally designed for the browser, now making waves in the cloud.  ## What is WebAssembly (Wasm)?  WebAssembly is a binary instruction format that enables code to run at near-native speed across different platforms. While it started as a way to run C/C++ and Rust in the browser, Wasm is now being adopted on the server for its speed, portability, and security.  **Key characteristics:** - **Performance:** Runs code at speeds close to native machine code - **Portability:** Works across browsers, servers, and edge devices - **Language Flexibility:** Supports C, C++, Rust, Go, .NET, Python, and more - **Security:** Executes in a sandboxed environment  ## Why Use Wasm in Serverless?  Serverless platforms are traditionally tied to specific languages (Node.js, Python, etc.) and can suffer from slow cold starts. Wasm changes the game by:  - **Reducing cold start times** due to its lightweight, fast-loading modules - **Enabling polyglot functions:** Write serverless code in many languages, not just JavaScript or Python - **Boosting performance:** Near-native execution for compute-heavy workloads - **Improving security:** Sandboxed execution reduces attack surface  ## How Does Wasm Work in Serverless?  Wasm modules are compiled code that can be executed by a Wasm runtime (like Wasmtime, Wasmer, or WasmEdge) on the server. Cloud providers and open-source projects are integrating Wasm into their platforms to let you deploy Wasm functions just like traditional serverless functions.  **Typical workflow:** 1. Write code in your favorite language (e.g., Rust, Go, C#) 2. Compile it to WebAssembly (.wasm file) 3. Deploy the Wasm module to a serverless platform that supports Wasm 4. The platform runs your Wasm code on demand, scaling automatically  ## Real-World Use Cases  - **API Gateways:** Use Wasm for high-performance request filtering and transformation - **Edge Computing:** Deploy Wasm modules close to users for ultra-low latency - **Machine Learning:** Run lightweight inference models in Wasm - **Custom Logic in SaaS:** Let users upload Wasm plugins to customize workflows - **IoT:** Use Wasm on resource-constrained devices for portability and safety  ## Example: Deploying a Wasm Function (Rust)  ```rust // src/lib.rs #[no_mangle] pub extern \"C\" fn add(a: i32, b: i32) -> i32 {     a + b } ```  Compile with: ```bash rustup target add wasm32-unknown-unknown cargo build --target wasm32-unknown-unknown --release ```  Upload the resulting `.wasm` file to your serverless provider (e.g., Cloudflare Workers, Fastly Compute@Edge, or Fermyon Spin).  ## Popular Wasm Serverless Platforms  - **Cloudflare Workers** ([docs](https://developers.cloudflare.com/workers/)) - **Fastly Compute@Edge** ([docs](https://developer.fastly.com/learning/compute/)) - **Fermyon Spin** ([docs](https://developer.fermyon.com/spin/)) - **AWS Lambda (Preview)** ([blog](https://aws.amazon.com/blogs/compute/introducing-the-aws-lambda-webassembly-runtime-preview/))  ## Best Practices  - **Optimize for size:** Smaller Wasm modules load faster - **Minimize dependencies:** Fewer dependencies mean better cold start times - **Test locally:** Use Wasm runtimes like Wasmtime or Wasmer for local testing - **Monitor performance:** Use platform tools to track execution time and resource usage  ## Challenges and Limitations  - **Limited access to OS features:** Wasm is sandboxed, so direct file/network access is restricted - **Ecosystem maturity:** Not all libraries are Wasm-ready - **Debugging:** Tooling is improving but not as mature as for native/server languages  ## The Future of Wasm in Serverless  Wasm is rapidly evolving, with growing support from cloud providers and open-source communities. Expect to see: - More language support - Better integration with serverless platforms - Expanded use cases (AI, streaming, real-time apps)  ## Conclusion  WebAssembly is unlocking new performance levels and possibilities for serverless computing. As the technology matures, expect Wasm to become a standard tool for building fast, portable, and secure cloud-native applications.  ---  *Want to learn more? Check out the [official WebAssembly documentation](https://webassembly.org/) and the resources from your favorite cloud provider.* "},{"slug":"you-are-doing-validation-wrong-in-net-code","title":"You’re Doing Validation Wrong in .NET","description":"Explore common mistakes and improved practices for validation in .NET applications.","tags":["Validation","C#","Best Practices","Code Quality","Error Handling"],"pubDate":"2024-10-18","content":" **Validation is a vital component in ensuring data integrity, but many developers follow inefficient or incorrect practices that can hurt performance and maintainability.**  ## Introduction  In this article, we explore common mistakes developers make when implementing validation in .NET applications. Validation is a vital component in ensuring data integrity, but many developers follow inefficient or incorrect practices that can hurt performance and maintainability. Let's delve into both bad and improved validation practices, along with code examples.  ## Main Content  ### Common Bad Practices  **1. Console Logging in Validation:** - **Example**: A validation method checks a user’s name, age, and email, logs an error to the console, and returns `false` if validation fails. - **Issue**: Logging to the console within a validation method mixes validation logic with output logic. This practice is inappropriate for non-console applications and reduces code clarity. Additionally, it can clutter the console output and make it harder to debug other parts of the application.  **2. Returning Enumerable of Strings:** - **Example**: Returning an `IEnumerable` with error messages when validation fails. - **Issue**: This method is inefficient as it creates a new list with each call and isn't intuitive for consumers to understand that an empty list means validation passed. It also makes it difficult to handle validation results consistently, as consumers need to check the list's contents rather than a simple boolean flag.  ### Sample Bad Validation Code  ```csharp public bool Validate(User user) {     if (string.IsNullOrEmpty(user.Name))     {         Console.WriteLine(\"Name cannot be empty\");         return false;     }     if (user.Age  120)     {         Console.WriteLine(\"Age must be between 18 and 120\");         return false;     }     if (string.IsNullOrEmpty(user.Email) || !user.Email.Contains(\"@\"))     {         Console.WriteLine(\"Email is not valid\");         return false;     }     return true; } ```  ### Preferred Validation Approach  **1. Fail Fast Approach:** This method allows the program to halt validation at the first encountered error, improving performance, especially when dealing with expensive operations like database queries. By stopping early, you avoid unnecessary checks and can provide immediate feedback to the user.  **2. Using Tuples for Validation Results:** Returning a tuple of `bool IsValid` and `IEnumerable Errors` ensures clearer feedback, making it easy to identify what went wrong during validation. This approach separates the validation logic from the error handling logic, making the code more modular and easier to maintain.  **3. Static Methods for Pure Functions:** Validation methods that do not alter object states should be static to improve predictability and ease of testing. Static methods are easier to test because they do not depend on the state of an instance, and they can be reused across different parts of the application.  ### Improved Validation Code  ```csharp public (bool IsValid, IEnumerable Errors) Validate(User user) {     var errors = new List();      if (string.IsNullOrEmpty(user.Name))     {         errors.Add(\"Name cannot be empty\");     }     if (user.Age  120)     {         errors.Add(\"Age must be between 18 and 120\");     }     if (string.IsNullOrEmpty(user.Email) || !user.Email.Contains(\"@\"))     {         errors.Add(\"Email is not valid\");     }      return (errors.Count == 0, errors); } ```  ### Advanced Functional Approach  For developers familiar with functional programming, using discriminated unions like Either monads in validation ensures cleaner error handling and early exits. This method returns either an error or the validated object, structuring the validation logic for better API integration. Discriminated unions provide a way to represent a value that can be one of several different types, making it easier to handle different validation outcomes.  **Example:**  ```csharp public Validation Validate(User user) {     if (string.IsNullOrEmpty(user.Name))     {         return Validation.Error(\"Name cannot be empty\");     }     if (user.Age  120)     {         return Validation.Error(\"Age must be between 18 and 120\");     }     if (string.IsNullOrEmpty(user.Email) || !user.Email.Contains(\"@\"))     {         return Validation.Error(\"Email is not valid\");     }     return Validation.Success(user); }  // Usage var result = Validate(user); result.Match(     success => Console.WriteLine(\"Validation passed\"),     error => Console.WriteLine($\"Validation failed: {error}\") ); ```  ## Conclusion  Effective validation in .NET requires a clean, efficient approach. Avoid bad practices such as mixing validation with output logic or relying on exception-driven control flow. By adopting better techniques like the Fail Fast approach, tuples for validation results, and functional programming principles, you can enhance the performance, maintainability, and clarity of your cod"}]